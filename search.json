[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R coding manual",
    "section": "",
    "text": "Hello! I’m Dr Richard M Timmerman. I was trained as an urban planner and designer and worked in various GIS/Surveying orientated government roles before returning to academia. It was while studying for my PhD that I was introduced to R by a very enthusiastic colleague. By the time I joined the University of Bristol, I had, for the most part, ditched using MS-Excel and MS-Word when writing papers and teaching materials for the far more flexible R.\nWhen I was first introduced to R, I was amused by the simplicity of its console and lack of a graphical user interface. Soon, I realised that simplicity gave complete control over data manipulation and analysis, and unparalled flexibility in terms of seamlessly weaving in graphical outputs and automatically updated text into my reports, saving me time and enhancing my accuracy when reporting on statistical trends."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Installing.html",
    "href": "Installing.html",
    "title": "Installing R",
    "section": "",
    "text": "It is difficult to articulate just how useful learning R is as university student. Therefore it is probably best for you to get started on it as soon as possible! A good starting point is getting R installed onto your machine.\n\n\nR can be downloaded by visiting the following link: https://cran.r-project.org/\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nFor a step by step guide on how to install R on to your respective operating system (Mac or Windows), can the QR code above or click here.\n\n\n\n\n\nInstalling R on your computer gives you access to its console and terminal. These are useful for quick queries or simple mathematical processes. However, as a Geographer, you will need to do far more with it. You will need to be able to developing a script for it, combining a sequence of steps and, later, to construct presentation graphics and reports from using its engine.\nThis is best done by downloading the RStudio IDE soon to be known as Posit.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nA step-by-step guide on how to install RStudio can be viewed by scanning the QR code below or by clicking [here] - pay attention to sections 1 and 2 in particular.\n\n\nBefore installing and launching RStudio, ensure that you have installed R beforehand. RStudio needs R in order to work. In addition to the console and terminal, RStudio includes various tabs that enable you to set working directories, save your data and outputs in a global environment, create scripts and, later, reports.\nThe you can install RStudio by clicking here."
  },
  {
    "objectID": "Intro.html",
    "href": "Intro.html",
    "title": "Overview",
    "section": "",
    "text": "Did you know that your laptop or computer really is keen to have a conversation with you? Often times, it asks you for certain things but you might not know exactly what it needs because its communicating in an entirely different language generically known as machine code. Fortunately, boffins from the dawn of computing developed languages that made it possible for the computer to talk to us with familiar language, where we can then reply using that same language. Sometimes the conversation is held in reverse, where we ask the computer for something or for a number of things. In essence, we give it a programme consisting of a list of items, not too disimilar to a what are handed at the start of theatrical performance. When dealing with computers, this is acheived using something called a ‘programming language’.\nSometimes, we might not want to attend a theatrical performance, but would rather visit a theme park that features shows at specific time intervals; duly, we are handed a programme that we are expected to follow in order to make it to the scheduled events. In computing terms, we might not necessary want to develop an app but would rather programme a game, or conduct some form of analysis to gain insights about some data set or another. It is the latter than interests us most!\nA popular programming language, that enables us to conduct statistical analyses on data is R.\n\n\nFirst released in 1993, R is a GNU project that is effectively an open source equivalent of another programming language known as S (developed in 1984). S is concerned with statistical processes in computing spheres and R adopts its essential framework. However, it’s creators, Ross Ihaka and Robert Gentleman, made it somewhat more powerful by enabling external programmers to develop packages that nuanced its applicability to a broader array of academic disciplines that routinely incorporated statistics into their practices. Geography is no exception here! Indeed there are packages that effectively transform R from a glorified calculator to an amazing Geographic Information System (GIS) suitable for geospatial analyses that often manifest in informative charts and maps.\nIn its most basic form, R contains a terminal and a console. This format easily fulfils its function as a calculator; however, to perform more elaborate processes, it is wise to make use of an Integrated Development Environment (IDE). IDEs offer the primary functionality of document creation in relation to the programming language, here it is R. The most well-known IDE for R is RStudio cloud. Crucially, RStudio allows you to save a list of items in your programme as a script, render outputs with a graphics engine, and produce publication ready outputs - reports, websites, and presentations.\n\n\n\nDuring your previous studies, you will have likely encountered some form of GIS software: ArcGIS, MapInfo, and QGIS to name a few. These will often feature a graphical user interface (GUI) that makes it easy to carry a range of analyses on geographic data. At first glance it would seem that R is out-competed by these software platforms. However, when focused on the aesthetics of a map or chart, the fundemental processes of how they are created are lost.\nR enforces a strict quantitative discipline when it comes to dealing with geographic data that is readily accepted accepted by the methodologies of all scientific disciplines that deal with numbers. Namely:\n\nDescriptive Statistics. These describe the core elements of the data such as its distribution, its mean, mode, and median. Additionally, variance between observations and thusly the likelihood of observing a statistic.\nInferential statistics. The process of making inferences from a quantitative data once an understanding of its distribution is established.\nSpatial Statistics. Upon exploring our data with descriptive and inferential test, we may be interested in examining whether spatial elements exist by adding spatial weightings to our statistics.\n\nBeing an open source statistics programming language, we are privy to the very latest statistical methods and innovations in GIS data processing. Further to this, and pertinent to the prevalence of big data, it is worth that R is capable of handling limitless data.\n\nExcel can contain up to ~ 1 million rows of data\nSPSS has a limit of ~ 2 billion rows\nR is unlimited!\n\nRobert Kabacoff (Kabacoff 2022) puts it best when he describes R as:\n\n“…a power platform for interaction data analysis and exploration…the results of any analytic step can be easilyt saved, manipulated, and used as input for additional analyses.” (p.5)."
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "R coding manual",
    "section": "2 Objectives",
    "text": "2 Objectives\nAs you read through this manual, it is hoped that you will also realise the benefits of R coding as you learn quantitative processes found in the geographical sciences.\nThe purpose of this manual is to introduce you to:\n\nIntroduce you to R\nGuide you through its installation process\nFamiliarise you with its base functions and its console\nIntroduce you to scripts and their pratical uses\nIntroduce you to R markdown"
  },
  {
    "objectID": "index.html#navigation",
    "href": "index.html#navigation",
    "title": "R coding manual",
    "section": "Navigation",
    "text": "Navigation\nYou can navigate to specific parts of this guide using the left-hand panel, where sub topics for each section can be viewed in the right-hand panel. The top-bar includes links to repositories and university resources that will aid with your learning.\n\n\n\n\n\n\nExercises\n\n\n\nTest"
  },
  {
    "objectID": "index.html#navigating-throught-the-manual",
    "href": "index.html#navigating-throught-the-manual",
    "title": "R coding manual",
    "section": "3 Navigating throught the manual",
    "text": "3 Navigating throught the manual\nYou can navigate to specific parts of this guide using the left-hand panel, where sub topics for each section can be viewed in the right-hand panel. The top-bar includes links to repositories and university resources that will aid with your learning."
  },
  {
    "objectID": "index.html#recommmended-reading",
    "href": "index.html#recommmended-reading",
    "title": "R coding manual",
    "section": "4 Recommmended Reading",
    "text": "4 Recommmended Reading\nThroughout this manual reference will be made to various literature resources. However, there are two principle resources that you are encouraged to read:\n\nRobert Kabacoff’s R in Action. This book goes into a significant detail concerning how to get started with R. Indeed, I would’ve submitted it with this manual were it not so dense. Nonetheless, a formidable supplemental read!\nRichard Harris and Claire Jarvis’s Statistics for Geography and Environmental Science. This is a fantastically clear book and particurlarly useful for those without a background in statistics. The overall structure of this manual loosely follows this book."
  },
  {
    "objectID": "Installing.html#the-programming-language",
    "href": "Installing.html#the-programming-language",
    "title": "Installing R",
    "section": "",
    "text": "R can be downloaded by visiting the following link: https://cran.r-project.org/\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nFor a step by step guide on how to install R on to your respective operating system (Mac or Windows), can the QR code above or click here."
  },
  {
    "objectID": "Installing.html#r-studio-posit",
    "href": "Installing.html#r-studio-posit",
    "title": "Installing R",
    "section": "R Studio (Posit)",
    "text": "R Studio (Posit)\nInstalling R on your computer gives you access to its console and terminal. These are useful for quick queries or simple mathematical processes. However, as a Geographer, you will need to do far more with it. You will need to be able to developing a script for it, combining a sequence of steps and, later, to construct presentation graphics and reports from using its engine.\nThis is best done by downloading the RStudio IDE soon to be known as Posit.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nA step-by-step guide on how to install RStudio can be viewed by scanning the QR code below or by clicking [here] - pay attention to sections 1 and 2 in particular.\n\n\nBefore installing and launching RStudio, ensure that you have installed R beforehand. RStudio needs R in order to work. In addition to the console and terminal, RStudio includes various tabs that enable you to set working directories, save your data and outputs in a global environment, create scripts and, later, reports.\nThe you can install RStudio by clicking here."
  },
  {
    "objectID": "Installing.html#posit-cloud-coding-with-friends",
    "href": "Installing.html#posit-cloud-coding-with-friends",
    "title": "Installing R",
    "section": "2.1 Posit Cloud (coding with friends)",
    "text": "2.1 Posit Cloud (coding with friends)\nPosit Cloud (formerly RStudio Cloud), is a peer-programming space gives you access to an already set up RStudio Environment via your web browser that you can invite others to view or enhance.\nHowever, there are distinct limitations regarding the synthesis of advanced graphical outputs, allocated computer processing unit (CPU) power, and time quotas. Many of these are negated by investing in a subscription that isn’t recommended unless you are working on a long-term project. Consider being frugal with regards to how much time you spend using it.\nIn theory, this version of R/RStudio can by used on a Chromebook.\nYou can explore this resource by clicking here. To register an account, you will ideally have a Gmail or GitHub account.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nA step-by-step guide on how to use Posit Cloud can be found by scanning the QR code above or by clicking here."
  },
  {
    "objectID": "Installing.html#rstudio-posit",
    "href": "Installing.html#rstudio-posit",
    "title": "Installing R",
    "section": "",
    "text": "Installing R on your computer gives you access to its console and terminal. These are useful for quick queries or simple mathematical processes. However, as a Geographer, you will need to do far more with it. You will need to be able to developing a script for it, combining a sequence of steps and, later, to construct presentation graphics and reports from using its engine.\nThis is best done by downloading the RStudio IDE soon to be known as Posit.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nA step-by-step guide on how to install RStudio can be viewed by scanning the QR code below or by clicking [here] - pay attention to sections 1 and 2 in particular.\n\n\nBefore installing and launching RStudio, ensure that you have installed R beforehand. RStudio needs R in order to work. In addition to the console and terminal, RStudio includes various tabs that enable you to set working directories, save your data and outputs in a global environment, create scripts and, later, reports.\nThe you can install RStudio by clicking here."
  },
  {
    "objectID": "RStudioInt.html",
    "href": "RStudioInt.html",
    "title": "Understanding RStudio",
    "section": "",
    "text": "RStudio is an Intergrated Development Environment (IDE). That is, it facilitates the development of data analyses, reports, and software based primarily on the R coding language. When required, it is able to incorporate syntaxes from other programming languages such as Python, Bash, CSS, LaTeX and others.\nUnlike the core R language, RStudio features a range of ergonomic and skeuomorphic features - buttons and menus that aid in the implementation of R in programming scenarios. However, relying on these features can actually make it harder to understand R when it comes to using more sophisticated processes. Therefore, to quickly become good at RStudio, you should stick to coding in the console or script as much as possible.\nBy default, the RStudio interface is divided into 3 panels\n\nThe console (along with the terminal and background jobs)\nThe Global Environment (along with History, Connections, Build, and Tutorials)\nThe Plots viewer (along with fields)\n\n\n\n\n\n\nFigure 1: The default RStudio Interface\n\n\n\n\nFor the time being, you only need to pay attention to the following parts of RStudio’s interface:\n\nThe console\nThe Environment\nThe Files tab\nThe Plots tab\n\n\n\nThe console contains a cursor after a &gt; symbol. Here, various lines of code can be entered directly into R - this feature is known as the command line prompt. It is often used to carry out quick operations.\nRemember the R language is primarily geared towards statistics. Therefore, at its heart, is a large and sophisticated calculator.\n\n\n\nLater in this manual, you will be asked to create a script that renders objects. These objects are essentially stored in the Global Environment, located in the Environment pane in RStudio.\n\n\n\nLocated in the bottom-right, The files window displays your file directory. It contains a range of icons that enable you create new folders within your file directory, add/delete/rename files, and also set your working directory.\n\n\n\n\n\n\nTip\n\n\n\nYou can click on the three dots ... towards the extreme right to manually navigate to an area where you want to save and/or access your files.\n\n\n\n\n\nThis is where any 2D/non-interactive graphics you produce in R will appear. You are able to save (or export) your graphics from this tab.\nInteractive and 3D graphics typically appear in the ‘viewer’ tab. This is a feature that is explored later in the manual."
  },
  {
    "objectID": "RCalculator.html",
    "href": "RCalculator.html",
    "title": "Using R as a calculator",
    "section": "",
    "text": "As R is a programming language, it responds to series of commands. Are written into the Console (underneath your Script pane). Primarily it can be used a calculator. For example:\n\n\nCode\n2 + 2\n\n\n[1] 4\n\n\nHere, the equation 2 + 2 is inputted into the console. To execute (or run) this, you can click anywhere along the line of code and click on the run button:\n\n\n\n\n\nAlternatively, you can hit the following combination of keys on your keyboard when your cursor is place anywhere along that line:\n\nWindows: Ctrl + Enter\nMac: Cmd + Enter (or Ctrl + Enter)\n\nNote that the result of the calculation appears in the console.\n\nAn advantage of using a script is that you are able input multiple lines of code and run them together. For example:\n\n\nCode\n2 * 2\n\n\n[1] 4\n\n\nCode\nsqrt(16)\n\n\n[1] 4\n\n\nCode\nabs(-4)\n\n\n[1] 4\n\n\nCode\nlog2(16)\n\n\n[1] 4\n\n\nRunning these together returns a list of outputs.\n\n\n\n\n\n\nTip\n\n\n\nsqrt(...), abs(...), and log2(...) are base functions featured in R. These and others will be discussed in more detail later in this guide.\n\n\n\n\nUse R to calculate the following equations:\n\n\\[\n2 \\times5+6\n\\]\n\\[\n30 \\times 30 \\times 60\n\\]\n\\[\n\\sqrt{\\frac{40}{12} \\times 50}\n\\]"
  },
  {
    "objectID": "RCalculator.html#brackets",
    "href": "RCalculator.html#brackets",
    "title": "Using R as a calculator",
    "section": "0.2 Brackets",
    "text": "0.2 Brackets\nBrackets play an import role in R. Fundamentally, they are applied to isolate a calculation or sequence. Consider the following example:\n\n2 + 2 * 4\n\n[1] 10\n\n\nVersus:\n\n(2 + 2) * 4\n\n[1] 16\n\n\nWhen a function such as sqrt is entered, it will contain brackets within which you can specify the values that you want to test. For example:\n\nsqrt(16) - sqrt(4)\n\n[1] 2"
  },
  {
    "objectID": "RCalculator.html#creating-a-vector",
    "href": "RCalculator.html#creating-a-vector",
    "title": "Using R as a calculator",
    "section": "3.1 Creating a vector",
    "text": "3.1 Creating a vector\nA vector is a string of values assigned to an object. It is a fundamental feature when it comes to statistical processes. To do this we need to introduce concatenation to denote a series of things, for instance a sequence of numbers. This is enabled using the concatenation function c(...). Here is an example of how it is applied to a series of numbers:\n\n\nCode\nmy_numerical_vector &lt;- c(1,2,3,4,5,6,7,8,9)\n\n\nAn example of when applied to a series of characters (text values); double or single quotations convert denote characters:\n\n\nCode\nmy_character_vector &lt;- c(\"Alice\", \"Bertha\", \"Collin\", \"David\", \"Ethan\", \"Francis\", \"Georgina\", \"Helen\", \"Iain\")\n\n\nThe examples given above contain a couple of other note worthy features.\n\nA designated name for your object. This is the my_vector part of the coding line.\nAn operator that looks like &lt;- that is interchangeable with = sign. Here, we are telling R that our object equates to our specified numerical (or string) vector.\n\n\n\n\n\n\n\nTip\n\n\n\nEach entry along the figure is separated with a comma ,\nWhenever you create an object, it appears in your the Global Environment located immediately to the left of your R Script window.\nWhen you combine numeric and string values into the same vector, its classification becomes ‘character’ rather than numeric.\nWhen the &lt;- operator points the other way (-&gt;), this denotes that code is being pushed towards an object; typically, this operator is preceded by a pipe %&gt;% operator that consists of a sequence of actions to be discussed later!."
  },
  {
    "objectID": "RCalculator.html#creating-a-matrix",
    "href": "RCalculator.html#creating-a-matrix",
    "title": "Using R as a calculator",
    "section": "3.2 Creating a Matrix",
    "text": "3.2 Creating a Matrix\nWhile a vector is a one-dimensional line of data inputs, a vector is two-dimensional - an array consisting of rows and columns. This differs from a convention Spreadsheet, such as those generated in Microsoft Excel, as calculations are not automatically generated.\n\n\n\n\nTable 1: Average temperature (ºC) recorded in Bristol, 2022\n\n\nmonths\ntemperature\n\n\n\n\nJan\n8\n\n\nFeb\n8\n\n\nMar\n11\n\n\nApr\n13\n\n\nMay\n17\n\n\nJun\n19\n\n\nJul\n22\n\n\nAug\n21\n\n\nSep\n18\n\n\nOct\n15\n\n\nNov\n11\n\n\nDec\n9\n\n\n\n\n\n\nTo create a matrix you’ll first need a two vectors. Let’s take the monthly average temperatures recorded in Waddington during 2020, 2022, and 2023 as an example @. The first vector, months, will contain abbreviated calendar months; the second vector will contain relative temperature (high)values in Celsius (ºC), see Table 1. The code for these is as follows:\n\n\nCode\nmonths &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nYear_2020 &lt;- c(9.2, 9.7, 10.6, 15.7, 18.4, 19.8, 20.2, 22.0, 18.9, 13.4, 11.4, 6.9)\nYear_2021 &lt;- c(5.1, 7.9, 11, 11.2, 14.5, 19.9, 22.5, 20.3, 20.7, 15.3, 10.6, 8.3)\nYear_2022 &lt;- c(7.6, 10.0, 12.1, 13.9, 17.8, 20.7, 24.5, 24.5, 19.0, 16.7, 11.3, 6.3)\n\n\nTo create the matrix, you can compile vectors using either the rbind(...) or the cbind(...) functions. Here is how cbind(...) is applied:\n\n\nCode\nTemperature_matrix &lt;- cbind(Year_2020, Year_2021, Year_2022)\nTemperature_matrix\n\n\n      Year_2020 Year_2021 Year_2022\n [1,]       9.2       5.1       7.6\n [2,]       9.7       7.9      10.0\n [3,]      10.6      11.0      12.1\n [4,]      15.7      11.2      13.9\n [5,]      18.4      14.5      17.8\n [6,]      19.8      19.9      20.7\n [7,]      20.2      22.5      24.5\n [8,]      22.0      20.3      24.5\n [9,]      18.9      20.7      19.0\n[10,]      13.4      15.3      16.7\n[11,]      11.4      10.6      11.3\n[12,]       6.9       8.3       6.3\n\n\nIf we were to use the rbind(...) function, the result is completely different. For example:\n\n\nCode\nTemperature_matrix_2 &lt;- rbind(Year_2020, Year_2021, Year_2022)\nTemperature_matrix_2\n\n\n          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nYear_2020  9.2  9.7 10.6 15.7 18.4 19.8 20.2 22.0 18.9  13.4  11.4   6.9\nYear_2021  5.1  7.9 11.0 11.2 14.5 19.9 22.5 20.3 20.7  15.3  10.6   8.3\nYear_2022  7.6 10.0 12.1 13.9 17.8 20.7 24.5 24.5 19.0  16.7  11.3   6.3\n\n\nThe method you prefer to use is entirely up to you. Here, it can be determined that the numerical values refer to a specific month in the year.\nIt is work noting how they are presented for example [1,] or [,1]. These appear in this manner are they indicate the dimension of the data, as we are dealing with two-dimensional data; there are two numbers present within the square brackets (crochets). As stated earlier, the two-dimensions present here are rows and columns, therefore, think of the brackets as showing [rows, columns], the number numbers act as coordinates for your data. Using the dim(...) function, it is possible to preview the dimensions of a respective matrix.\n\n\nCode\ndim(Temperature_matrix)\n\n\n[1] 12  3\n\n\nFrom this result, we learn that the matrix has 12 rows and 3 columns.\nIf you wanted to retrieve the 5th row for Waddington’s 2021 temperature data, you can write the following the console for the first matrix:\n\n\nCode\nTemperature_matrix[5,\"Year_2021\"]\n\n\nYear_2021 \n     14.5 \n\n\nFor the second matrix…\n\n\nCode\nTemperature_matrix_2[\"Year_2021\", 5]\n\n\nYear_2021 \n     14.5 \n\n\nThis process of previewing the data is beginning of what is referred to as ‘slicing’. This process will be covered in more detail in the next section of this resource.\n\n3.2.1 Assigning row/column names\nThe matrixes we have created are missing the names of the months. These are instead numbers that aren’t particularly useful to someone seeing this data for the first time, and out of context. To address this issue, we can using the either the row.names(...) or colnames(...) functions to address this flaw.\nFor the first matrix, Temperature_matrix, the row.names(...) function can be applied using the months vector that was created earlier. The method is as follows:\n\n\nCode\nrow.names(Temperature_matrix) &lt;- months\nTemperature_matrix\n\n\n    Year_2020 Year_2021 Year_2022\nJan       9.2       5.1       7.6\nFeb       9.7       7.9      10.0\nMar      10.6      11.0      12.1\nApr      15.7      11.2      13.9\nMay      18.4      14.5      17.8\nJun      19.8      19.9      20.7\nJul      20.2      22.5      24.5\nAug      22.0      20.3      24.5\nSep      18.9      20.7      19.0\nOct      13.4      15.3      16.7\nNov      11.4      10.6      11.3\nDec       6.9       8.3       6.3\n\n\nFor the second matrix, Temperature_matrix_2, the column names are missing and are corrected with the following:\n\n\nCode\ncolnames(Temperature_matrix_2) &lt;- months\nTemperature_matrix_2\n\n\n          Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov Dec\nYear_2020 9.2  9.7 10.6 15.7 18.4 19.8 20.2 22.0 18.9 13.4 11.4 6.9\nYear_2021 5.1  7.9 11.0 11.2 14.5 19.9 22.5 20.3 20.7 15.3 10.6 8.3\nYear_2022 7.6 10.0 12.1 13.9 17.8 20.7 24.5 24.5 19.0 16.7 11.3 6.3\n\n\n\n\n3.2.2 Renaming columns/rows\nThe names in the Temperature_matrix aren’t well formatted (i.e. Year_2020) - we want to remove the underscore between Year and 2020, and do the same for the other columns. Here, we can use the colnames(...) function to override these names. The process involves the manipulation of the matrix’s dimensions and is as follows:\n\n\nCode\ncolnames(Temperature_matrix)[1] &lt;- \"Year 2020\"\ncolnames(Temperature_matrix)[2] &lt;- \"Year 2021\"\ncolnames(Temperature_matrix)[3] &lt;- \"Year 2022\"\n\nTemperature_matrix\n\n\n    Year 2020 Year 2021 Year 2022\nJan       9.2       5.1       7.6\nFeb       9.7       7.9      10.0\nMar      10.6      11.0      12.1\nApr      15.7      11.2      13.9\nMay      18.4      14.5      17.8\nJun      19.8      19.9      20.7\nJul      20.2      22.5      24.5\nAug      22.0      20.3      24.5\nSep      18.9      20.7      19.0\nOct      13.4      15.3      16.7\nNov      11.4      10.6      11.3\nDec       6.9       8.3       6.3\n\n\nTo change the row names, the process is similar but relies on the row.names(....) function. However, this time, its worth giving the concatenation function a go to produce a more concise line of code:\n\n\nCode\nrow.names(Temperature_matrix_2) &lt;- c(\"Year 2020\", \"Year 2021\", \"Year 2022\")\nTemperature_matrix_2\n\n\n          Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov Dec\nYear 2020 9.2  9.7 10.6 15.7 18.4 19.8 20.2 22.0 18.9 13.4 11.4 6.9\nYear 2021 5.1  7.9 11.0 11.2 14.5 19.9 22.5 20.3 20.7 15.3 10.6 8.3\nYear 2022 7.6 10.0 12.1 13.9 17.8 20.7 24.5 24.5 19.0 16.7 11.3 6.3\n\n\n\n\n\n\n\n\nTip\n\n\n\nRenaming columns and rows can be carried out in a variety of ways. The method described above is the base approach, later, similar processes will be carried out using tidy methodical approaches."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "R coding manual",
    "section": "",
    "text": "Hello! I’m Dr Richard M Timmerman. I was trained as an urban planner and designer and worked in various GIS/Surveying orientated government roles before returning to academia. It was while studying for my PhD that I was introduced to R by a very enthusiastic colleague. By the time I joined the University of Bristol, I had, for the most part, ditched using MS-Excel and MS-Word when writing papers and teaching materials for the far more flexible R.\nWhen I was first introduced to R, I was amused by the simplicity of its console and lack of a graphical user interface. Soon, I realised that simplicity gave complete control over data manipulation and analysis, and unparalled flexibility in terms of seamlessly weaving in graphical outputs and automatically updated text into my reports, saving me time and enhancing my accuracy when reporting on statistical trends."
  },
  {
    "objectID": "Intro.html#what-is-r",
    "href": "Intro.html#what-is-r",
    "title": "Overview",
    "section": "",
    "text": "First released in 1993, R is a GNU project that is effectively an open source equivalent of another programming language known as S (developed in 1984). S is concerned with statistical processes in computing spheres and R adopts its essential framework. However, it’s creators, Ross Ihaka and Robert Gentleman, made it somewhat more powerful by enabling external programmers to develop packages that nuanced its applicability to a broader array of academic disciplines that routinely incorporated statistics into their practices. Geography is no exception here! Indeed there are packages that effectively transform R from a glorified calculator to an amazing Geographic Information System (GIS) suitable for geospatial analyses that often manifest in informative charts and maps.\nIn its most basic form, R contains a terminal and a console. This format easily fulfils its function as a calculator; however, to perform more elaborate processes, it is wise to make use of an Integrated Development Environment (IDE). IDEs offer the primary functionality of document creation in relation to the programming language, here it is R. The most well-known IDE for R is RStudio cloud. Crucially, RStudio allows you to save a list of items in your programme as a script, render outputs with a graphics engine, and produce publication ready outputs - reports, websites, and presentations."
  },
  {
    "objectID": "Intro.html#why-use-r",
    "href": "Intro.html#why-use-r",
    "title": "Overview",
    "section": "",
    "text": "During your previous studies, you will have likely encountered some form of GIS software: ArcGIS, MapInfo, and QGIS to name a few. These will often feature a graphical user interface (GUI) that makes it easy to carry a range of analyses on geographic data. At first glance it would seem that R is out-competed by these software platforms. However, when focused on the aesthetics of a map or chart, the fundemental processes of how they are created are lost.\nR enforces a strict quantitative discipline when it comes to dealing with geographic data that is readily accepted accepted by the methodologies of all scientific disciplines that deal with numbers. Namely:\n\nDescriptive Statistics. These describe the core elements of the data such as its distribution, its mean, mode, and median. Additionally, variance between observations and thusly the likelihood of observing a statistic.\nInferential statistics. The process of making inferences from a quantitative data once an understanding of its distribution is established.\nSpatial Statistics. Upon exploring our data with descriptive and inferential test, we may be interested in examining whether spatial elements exist by adding spatial weightings to our statistics.\n\nBeing an open source statistics programming language, we are privy to the very latest statistical methods and innovations in GIS data processing. Further to this, and pertinent to the prevalence of big data, it is worth that R is capable of handling limitless data.\n\nExcel can contain up to ~ 1 million rows of data\nSPSS has a limit of ~ 2 billion rows\nR is unlimited!\n\nRobert Kabacoff (Kabacoff 2022) puts it best when he describes R as:\n\n“…a power platform for interaction data analysis and exploration…the results of any analytic step can be easilyt saved, manipulated, and used as input for additional analyses.” (p.5)."
  },
  {
    "objectID": "RStudioInt.html#the-console",
    "href": "RStudioInt.html#the-console",
    "title": "Understanding RStudio",
    "section": "",
    "text": "The console contains a cursor after a &gt; symbol. Here, various lines of code can be entered directly into R - this feature is known as the command line prompt. It is often used to carry out quick operations.\nRemember the R language is primarily geared towards statistics. Therefore, at its heart, is a large and sophisticated calculator."
  },
  {
    "objectID": "RStudioInt.html#the-environment",
    "href": "RStudioInt.html#the-environment",
    "title": "Understanding RStudio",
    "section": "",
    "text": "Later in this manual, you will be asked to create a script that renders objects. These objects are essentially stored in the Global Environment, located in the Environment pane in RStudio."
  },
  {
    "objectID": "RStudioInt.html#the-files-tab",
    "href": "RStudioInt.html#the-files-tab",
    "title": "Understanding RStudio",
    "section": "",
    "text": "Located in the bottom-right, The files window displays your file directory. It contains a range of icons that enable you create new folders within your file directory, add/delete/rename files, and also set your working directory.\n\n\n\n\n\n\nTip\n\n\n\nYou can click on the three dots ... towards the extreme right to manually navigate to an area where you want to save and/or access your files."
  },
  {
    "objectID": "RStudioInt.html#the-plots-tab",
    "href": "RStudioInt.html#the-plots-tab",
    "title": "Understanding RStudio",
    "section": "",
    "text": "This is where any 2D/non-interactive graphics you produce in R will appear. You are able to save (or export) your graphics from this tab.\nInteractive and 3D graphics typically appear in the ‘viewer’ tab. This is a feature that is explored later in the manual."
  },
  {
    "objectID": "Classical_Measures.html",
    "href": "Classical_Measures.html",
    "title": "Classical Measures",
    "section": "",
    "text": "The previous section in this website introduced you to summary statistics. Examining these in R allows you to make early descriptions of your data. The data featured below shows 99 randomly generated numbers between 1 and 64 using the runif(...) function:\nCode\nset.seed(12)\n\nsome_data &lt;- runif(n = 99, min = 1, max = 64)\nprint(some_data)\n\n\n [1]  5.369738 52.519838 60.385169 17.971058 11.668932  3.135424 12.263455\n [8] 41.424918  2.441298  1.524464 25.739923 52.274475 24.703653 24.991168\n[15] 17.689858 28.678062 29.829251 35.064575 42.937829  8.100033 14.757132\n[22] 50.633690  7.164741 45.719319 14.722852 17.880446 32.800381 12.880976\n[29] 28.684047 43.198616 16.175644 57.275688 56.613652 52.285985 40.895670\n[36] 60.288515 44.716420 54.153241 25.232601 25.641187 38.074355 34.420083\n[43] 62.591061 13.451925 54.186237  6.986759 25.012513  3.890700 10.615302\n[50] 49.807246 25.858946 23.775325 27.528673 21.286726 35.786132 62.189724\n[57] 36.151500 37.171130 41.404934 52.908786 48.239816 28.383821 62.794743\n[64] 36.019185 19.537367 11.054198  9.958532 48.377683 23.350914 49.127825\n[71] 20.768646 46.030605 39.320316 25.172236 20.894582 61.059193 50.618505\n[78] 45.212050 10.927577 52.380595 35.133768 56.301209  8.802048 16.294423\n[85] 46.260043 17.514489  4.483030 56.304066 24.867523  7.412225 43.427489\n[92] 10.727022 11.349098 24.085778 37.246710 32.193896 49.368178 24.419172\n[99] 20.602320\nTo find the average number from this series, the function mean(...) can be used on the data. For example:\nCode\nmean(some_data)\n\n\n[1] 31.30861\nThere are various classical measures used in statistics to describe data and these will be discussed in this section. These include averages, central tendency, variability, scatter, and variation."
  },
  {
    "objectID": "Classical_Measures.html#the-median",
    "href": "Classical_Measures.html#the-median",
    "title": "Classical Measures",
    "section": "1.1 The median",
    "text": "1.1 The median\nFrom the stem and leaf plot featured in Section 1, it is possible to calculate the middle point of the data known as the median. Equation is as follows:\n\\[\n\\mu = X\\begin{bmatrix}\\frac{n+1}{2}\\end{bmatrix}\n\\tag{1}\\]\nHere, \\(\\mu\\) represents the median (the value), \\(X\\) is the ordered list of numbers (as featured in the Stem and Leaf plot), \\(n\\) is the number of observations.\nThe process is involves counting the number of observations plus 1 and dividing that number by 2. The data we’ve been examining so far has 99 observations (\\(n\\) = 99), so we can begin to solve the equation:\n\\[\n\\begin{align}\n\\mu & = X\\begin{bmatrix}\\frac{99+1}{2}\\end{bmatrix}\n\\\\\n\\\\\n& = X[50]\n\\end{align}\n\\tag{2}\\]\nTherefore, at our 50th observation, we’ll find our median. Doing this from the Stem and Leaf plot in Section 1 gives us the value of approximately (~) 20.9. However, with R, we can obtain a more precise figure using the median(...) function. For example:\n\n\nCode\nmedian(x = some_data)\n\n\n[1] 28.67806\n\n\n\n\n\n\n\n\nTip\n\n\n\nCalculating the median by hand for even data…\nThe \\(\\mu\\) process above works well with an odd number of observations. However, it differences slightly when dealing with an even number of observations, where the equation becomes (\\(\\mu'\\)):\n\\[\n\\mu' = \\frac{X\\begin{bmatrix}\\frac{n}{2}\\end{bmatrix}+X{\\begin{bmatrix}\\frac{n}{2}+1\\end{bmatrix}}}{2}\n\\tag{3}\\]\nThat is to say, you first need to find the divide your the number of your observation into two and add that value to your previous previous result with an additional unit. This is then divided into two. For 100 observations:\n\\[\n\\begin{align}\n\\mu' & = \\frac{X\\begin{bmatrix}\\frac{100}{2}\\end{bmatrix}+X{\\begin{bmatrix}\\frac{100}{2}+1\\end{bmatrix}}}{2}\n\\\\\n\\\\\n& = \\frac{X\\begin{bmatrix}50\\end{bmatrix}+X{\\begin{bmatrix}50+1\\end{bmatrix}}}{2}\n\\\\\n\\\\\n& = X\\begin{bmatrix}\\frac{101}{2}\\end{bmatrix}\n\\\\\n\\\\\n& = X[50.5]\n\\end{align}\n\\tag{4}\\]\nThe median will lie between the 50th and 51st observations. In R, the median(...) function works for both even and uneven vector sizes."
  },
  {
    "objectID": "Classical_Measures.html#comparing-distributions",
    "href": "Classical_Measures.html#comparing-distributions",
    "title": "Classical Measures",
    "section": "1.2 Comparing distributions",
    "text": "1.2 Comparing distributions\nThere may be instances, where you’d want to compare distributions. For example, One set of randomly regenerated data vs. another. As R isn’t able to do this on its own, this process can be achieved by using the stem.leaf.backback(...) function from the aplpack package.\nLet’s use it to compare the results of two randomly sampled data values (\\(n\\) = 99) for set at different seeds…\n\n\nCode\n# To load the package in R type...\nchooseCRANmirror(ind = 68)\nif(!require(aplpack)) install.packages(\"aplpack\", dependencies = TRUE)\n\n# Calibrating the comparative data\nset.seed(13)\nmore_data &lt;- runif(99, min = 1, max = 64) \n\n# rendering the result\nstem.leaf.backback(x = some_data, y = more_data)\n\n\n_______________________________________________\n  1 | 2: represents 12, leaf unit: 1 \n            some_data      more_data        \n_______________________________________________\n    5           43321| 0* |122234          6   \n   12         9887765| 0. |56666779       14   \n   23     44322111000| 1* |000133         20   \n   30         9777766| 1. |5556899        27   \n   41     44444331000| 2* |111123334      36   \n  (11)    98887555555| 2. |5566667789999  49   \n   47             422| 3* |0024           (4)  \n   44       987766555| 3. |5556677888999  46   \n   35         4332110| 4* |000112333      33   \n   28       999886655| 4. |56789          24   \n   19       442222200| 5* |111            19   \n   10            7666| 5. |566788899      16   \n    6          222100| 6* |0001133         7   \n                     | 6. |                    \n                     | 7* |                    \n_______________________________________________\nn:                 99      99               \n_______________________________________________\n\n\nThe results are a little more complex but consistent with Tukey’s notation (1977), where he describes the process as scratching down numbers, to get a feel of what they like (ibid.). Here is single (*) is a one-digit leaf - a single decimal point - for the starting half of rounded numbers: .0 - .4; the (.) cover the remaining half of the rounded values: .5 - .9. these are the extreme columns are the sum totals towards the median of the data for each row of observations (left and right). Again, this gives a good indication distribution’s shape. Here, We can see the medians differ slightly for each vector. For ‘some_data’, the median is median(some_data), for ‘more data’, median(more_data). From the graphic, we learn that these figures can be found at around the 11th observation of the range of 25-29 (.) for ‘some_data’, while the median is located at 4th count of the of the range between 30 and 34 (*). Towards the bottom, are are the total number of observations for each variable."
  },
  {
    "objectID": "Descriptive_Statistics.html",
    "href": "Descriptive_Statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "In the previous section, using R as a Calculator, introduced summary statistics in R. The outputs of the summary(...) function in R provides readily useful data insights including the:\nWhen dealing with numerical data, it also gives insights regarding quartile breaks.\nThe function behaves differently when processing non-numeric data, where it will return metrics concerning the frequency of occurrences, field length and class. For now, let’s focus on unpacking the numerical qualities."
  },
  {
    "objectID": "Descriptive_Statistics.html#the-purpose-of-quantification",
    "href": "Descriptive_Statistics.html#the-purpose-of-quantification",
    "title": "Descriptive Statistics",
    "section": "1 The purpose of quantification",
    "text": "1 The purpose of quantification\nAccording to Richard Harris and Claire Jarvis, “data collection and analysis are central to the functioning of contemporary society…data handling of statistics is a necessary skill to social and scientific debate” (Harris and Jarvis 2014). Once data has been gathered, the next task is to make sense of it. A simple example could be: having just finished asking 100 people what their favourite colour is, how would you know which colour is most popular?\nStatistics is derived from the Latin word ‘Statisticum’ that translates to ‘of the government’ or ‘of the state’. Therefore, the study of statistics literally means the study of the state, or to analyse something to do with the state. In the simple example given above, the state is the 100 surveyed people. The mission becomes to analyse the 100 people."
  },
  {
    "objectID": "Descriptive_Statistics.html#geographic-data-frames-gdf",
    "href": "Descriptive_Statistics.html#geographic-data-frames-gdf",
    "title": "Descriptive Statistics",
    "section": "2 Geographic Data Frames (GDF)",
    "text": "2 Geographic Data Frames (GDF)\nA data frame is a the next step up from a matrix. While a matrix contains a single class of data (i.e. purely numeric), a data frame can contain difference classes of data variable. An example of a data frame is noted below along with a test of the object’s class. The data shows the effect of vitamin C on the growth rate of teeth for the first 6 rows of 60 observations.\n\n\nCode\nhead(x = ToothGrowth, n = 10)\n\n\n    len supp dose\n1   4.2   VC  0.5\n2  11.5   VC  0.5\n3   7.3   VC  0.5\n4   5.8   VC  0.5\n5   6.4   VC  0.5\n6  10.0   VC  0.5\n7  11.2   VC  0.5\n8  11.2   VC  0.5\n9   5.2   VC  0.5\n10  7.0   VC  0.5\n\n\nCode\nclass(x = ToothGrowth)\n\n\n[1] \"data.frame\"\n\n\nA geographic data frame differs from a conventional data frame as it includes geographic data such as geometries, longitude, and latitude. Geographic data fields, make it possible to spatialise and map your data.\n\n\n\n\n\n\nClass in data objects\n\n\n\nClass is a shortened handle for classification. R is able to identify a range of different classes either via the summary(...) or class(...) functions. Knowing the class of your data will help you to select the most appropriate instruments for analysis."
  },
  {
    "objectID": "Classical_Measures.html#when-to-use-a-stem-leaf-plot",
    "href": "Classical_Measures.html#when-to-use-a-stem-leaf-plot",
    "title": "Classical Measures",
    "section": "1.3 When to use a Stem & Leaf plot",
    "text": "1.3 When to use a Stem & Leaf plot\nYou will find it appropriate to use a Stem & Leaf plot when you need to:\n\nQuickly determine the frequency of an observation\nQuickly order the observations in your data.\nQuickly visualise the distribution of your data\nQuickly make and report on findings while in the field\nRequire more information than a what is present on a histogram.\n\n\n\n\n\n\n\nReminder on loading packages\n\n\n\nThe first line of code chooseCRANmirrow(ind = 68) specifies where the package will be downloaded from. CRAN stands for the *Comprehensive R Archive Network, and the mirror is the exit-node you’ll be using. There are over 100 mirrors, but I normally settle on 68th index (ind) by habit; if you cannot find you package, you should consider changing the ind value.\nThe second line of the code actually loads the package. Conventionally, you can load a package by typing the following into a blank script.\ninstall.packages(\"aplpack\")\nlibrary(aplpack)\nAlternatively:\ninstall.packages(\"aplpack\", repos = \"https://cloud.r-project.org\")\nrequire(aplpack)\nWe’ll revisit R packages in a later section of this website.\n\n\nIt’s worth noting that Stem & Leaf plots can be used with time series and non-numeric data but for the purposes of this tutorial, we’ll stop at it’s simple application on numerical vectors."
  },
  {
    "objectID": "Descriptive_Statistics.html#statistical-summaries",
    "href": "Descriptive_Statistics.html#statistical-summaries",
    "title": "Descriptive Statistics",
    "section": "3 Statistical Summaries",
    "text": "3 Statistical Summaries"
  },
  {
    "objectID": "Descriptive_Statistics.html#minimum-and-maximum-values",
    "href": "Descriptive_Statistics.html#minimum-and-maximum-values",
    "title": "Descriptive Statistics",
    "section": "4 Minimum and Maximum values",
    "text": "4 Minimum and Maximum values\nThe minimum (\\(\\wedge\\)) and maximum values (\\(\\vee\\))are self-explanatory. They represent the lowest and highest value present in your data vector/variable. These can be found easily in R using the min(...) and max(...) functions. For example, using the mtcars data set, we can find the minimum horsepower (hp) for the featured 1970s cars using the following lines of code:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nThe maximum horsepower is quickly retrieved using the following:\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\n\n4.1 The arithmetic mean\nAs discussed in a section ‘R as a Calculator’, the arithmetic mean is an average. It’s equation is as follows:\n\\[\n\\bar{x} = \\frac{\\sum{x_i}}{n}\n\\] Here, the \\(\\bar{x}\\) represents the calculated mean (average), the \\(\\sum\\) means the sum total of something, \\(x_i\\) represents your numerical data vector/variable - a single index (\\(i\\)), and \\(n\\) is number of observations in your vector/variable. Therefore, the mean can be described as the sum total of a numerical string divided by its length.\nTo help illustrate this point, imagine you have a vector that looks is as follows: 1, 2, 3, 4, 5, 6. Working out the mean can be achieved in the following way:\n\\[\n\\begin{align}\n\\bar{x} & = \\frac{1+2+3+4+5+6}{6}\n\\\\\n\\\\\n& = \\frac{21}{6}\n\\\\\n\\\\\n& = 3.5\n\\end{align}\n\\]\nThe mean can be considered as a measure of centrality, a concept that will be expanded on in the next section. Otherwise, it is a useful tool in determining the average from a distribution of values. It is worth noting that the values do not need to be ordered. In R it is easily applied using the mean(...) function. For example:\n\n\nCode\nmean(c(1,2,3,4,5,6))\n\n\n[1] 3.5\n\n\n\n\n4.2 The weighted mean\nSometimes, you’ll face situations where you are dealing with means across an array of columns. For example, your vector can comprise of values related to daily temperatures. The data is as follows:\n\n\nCode\n# Generating data\nweek_days &lt;- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\ndaily_temp &lt;- c(19, 19, 20, 19, 21, 20, 20)\n\nSamp_temp &lt;- data.frame(week_days, daily_temp)\n\n# Renaming columns\nnames(Samp_temp)[names(Samp_temp) == \"week_days\"] &lt;- \"Day\"\nnames(Samp_temp)[names(Samp_temp) == \"daily_temp\"] &lt;- \"Temperature\"\n\n# Plotting transposed (t) table\nknitr::kable(t(Samp_temp))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nSunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\nTemperature\n19\n19\n20\n19\n21\n20\n20\n\n\n\n\n\nBecause we’re cooped up indoors for the majority of the week, we might weight our opinions of weather of this week was ‘good’ or ‘bad’ based on the weekends. In this case, a weighted mean can be a helpful tool. It’s equation is as follows:\n\\[\n\\bar{x}_w = \\frac{\\sum_{i=1}^{n}(x_i \\cdot w_i)}{\\sum_{i=1}^{n}w_i} \\ \\ \\ \\ \\therefore \\ \\ \\ \\ \\bar{x}_w = \\frac{\\sum{wx}}{\\sum{w}}\n\\]\nHere, \\(w\\) represents your weights; as before, \\(x_i\\) are your indexed values. This formula may seem intimidating, however, all it is doing is allowing you to increase the importance of certain values other another by adjusting their multiplication factors. The arithmetic mean places equal weightings to observations so that the even weighting is \\(\\frac{1}{7} \\approx 0.142\\) for each day of the week; tallied up, their value will be \\(1\\):\n\n\nCode\n# Manually inputted original weightings (approximated)\nSamp_temp$original_weight &lt;- c(0.142, 0.142, 0.142, 0.142, 0.142, 0.142, 0.142)\n\n# Output table\nknitr::kable(t(Samp_temp[,c(1,3)]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nSunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\noriginal_weight\n0.142\n0.142\n0.142\n0.142\n0.142\n0.142\n0.142\n\n\n\n\n\nIf we wanted to place emphasis on Saturday and Sunday, we’d override the weights with higher decimal values placed on these days - ensuring that the total still comes up to \\(1\\):\n\n\nCode\n# Manually inputted original weightings (approximated)\nSamp_temp$adjusted_weight &lt;- c(0.1, 0.1, 0.1, 0.1, 0.1, 0.25, 0.25)\n\n# Output table\nknitr::kable(t(Samp_temp[,c(1,4)]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nSunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\nadjusted_weight\n0.10\n0.10\n0.10\n0.10\n0.10\n0.25\n0.25\n\n\n\n\n\nWith these values, it is possible to calculate the weighted mean using the equation above:\n\\[\\begin{align}\n\\bar{x}_w & = \\frac{\\sum{(1.9, 1.9, 2, 1.9, 2.1, 5, 5)}} {(0.1, 0.1, 0.1, 0.1, 0.1, 0.25, 0.25)}\n\\\\\n\\\\\n& = \\frac{19.8 \\ \\cdot \\ 1}{1}\n\\\\\n\\\\\n& = 19.8\n\n\\end{align}\\] \\end{equation}\nIn R, the weighted.mean(...) function can be used calculate this value. Using our data frame Samp_temp the process is as follows:\n\n\nCode\nweighted.mean(x = Samp_temp$Temperature, w = Samp_temp$adjusted_weight)\n\n\n[1] 19.8\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible to generate rows of weighted means based on column values. However, this process requires the use of R’s apply(...) function in order to evaluate the array. The approximate format is as follows:\n\n\nCode\nyour_data$w_avg &lt;- apply(X = your_data,\n                         MARGIN = 1, \n                         FUN = function(x) weighted.mean(x = x[1:nrow(your_data), w = adjusted_column_weights))\n\n\n\n4.2.1 Description:\nIn your data frame, create a new column that returns the weighted average. This new column will feature a weighted average per row of data. X = is specifies that you intend the column to fit into your data frame; MARGIN = 1 tells R that we want to calculate values for each row of data present; FUN = function(x) applies the weighted.mean(...) function to each data entry - the range being from the first entry to the last, denoted by nrow(your_data). For this last part, the x = and y = arguments are called in from the weighted.mean(...) function.\nRemember, its good practice to have your adjusted weights equal to exactly 1!."
  },
  {
    "objectID": "Descriptive_Statistics.html#the-mode",
    "href": "Descriptive_Statistics.html#the-mode",
    "title": "Descriptive Statistics",
    "section": "5 The mode",
    "text": "5 The mode\nEssentially, the mode (\\(M_o\\)) is easily grasped - it tells us the value that occurs most frequently (\\(f\\)), However, its formula can take some time to digest:\n\\[\\begin{equation}\nM_o = l + h\n\\begin{pmatrix}\n\\frac{f_m - f_1}{2f_m - f_1 - f_2}\n\\end{pmatrix}\n\\end{equation}\\]\nAlso, here is a vector showing the number of gears in the cars surveyed in the mtcars data set.\n\n\nCode\nmtcars$gear\n\n\n [1] 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4\n\n\nIn the, \\(l\\) refers to the lower boundary of the modal class. Eyeballing the vector above, we can determine that this value is 3; we can also use the min(...) function to discover this. \\(h\\) refers to the size of the vectors we’re analysing (equivalent to \\(n\\)) - to find this, we can use the length(...) function in R; \\(f_m\\) refers to the frequency corresponding to a modal class (the number of 3s, 4s, and 5s); \\(f_1\\) is the preceding, to a modal class - for 3s this will be 0, for the 4s it will be number of 3s, etc; \\(f_2\\) is the number proceeding to the next modal class - for the 4s this will be the number of 5s.\nSolving the formula by hand is lengthy process, so I won’t stress the details! Instead, it’s worth noting that R does not have a native function for this, you will need to build one from scratch(!); functions will be explained in more detail later in this website. For now let’s try the following based on code shared in Statology:\n\n\nCode\ncalc_mode &lt;- function(x) {\n  u &lt;- unique(x)\n  tab &lt;- tabulate(match(x, u))\n  u[tab == max(tab)]\n}\n\n\nThe key thing to note here is that there only one piece of information is required here, your vector x. This function uses the unique(...) function that detects unique values and ignores duplications; tabulate(...) calculates the number of times a value appears against a unique entry - this is helped by the match(...) function. The returned result, is the maximum value (note the max(...) function) is the most frequently occurring value detected in the tabulation process.\nWhen the function is applied to the mtcars$gear vector, the result is as follows:\n\n\nCode\ncalc_mode(mtcars$gear)\n\n\n[1] 3\n\n\nThe importance of the mode is important for the next section as it can help to detect centrality."
  },
  {
    "objectID": "Descriptive_Statistics.html#numerical-summaries",
    "href": "Descriptive_Statistics.html#numerical-summaries",
    "title": "Descriptive Statistics",
    "section": "3 Numerical Summaries",
    "text": "3 Numerical Summaries\n\n3.1 Minimum and Maximum values\nThe minimum (\\(\\wedge\\)) and maximum values (\\(\\vee\\))are self-explanatory. They represent the lowest and highest value present in your data vector/variable. These can be found easily in R using the min(...) and max(...) functions. For example, using the mtcars data set, we can find the minimum horsepower (hp) for the featured 1970s cars using the following lines of code:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nThe maximum horsepower is quickly retrieved using the following:\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\n\n\n3.2 The arithmetic mean\nAs discussed in a section ‘R as a Calculator’, the arithmetic mean is an average. It’s equation is as follows:\n\\[\n\\bar{x} = \\frac{\\sum{x_i}}{n}\n\\] Here, the \\(\\bar{x}\\) represents the calculated mean (average), the \\(\\sum\\) means the sum total of something, \\(x_i\\) represents your numerical data vector/variable - a single index (\\(i\\)), and \\(n\\) is number of observations in your vector/variable. Therefore, the mean can be described as the sum total of a numerical string divided by its length.\nTo help illustrate this point, imagine you have a vector that is as follows: 1, 2, 3, 4, 5, 6. Working out the mean can be achieved in the following way:\n\\[\n\\begin{align}\n\\bar{x} & = \\frac{1+2+3+4+5+6}{6}\n\\\\\n\\\\\n& = \\frac{21}{6}\n\\\\\n\\\\\n& = 3.5\n\\end{align}\n\\]\nThe mean can be considered as a measure of centrality, a concept that will be expanded on in the next section. Otherwise, it is a useful tool in determining the average from a distribution of values. It is worth noting that the values do not need to be ordered. In R it is easily applied using the mean(...) function. For example:\n\n\nCode\nmean(c(1,2,3,4,5,6))\n\n\n[1] 3.5\n\n\n\n\n3.3 The weighted mean\nSometimes, you’ll face situations where you are dealing with means across an array of columns. For example, your vector can comprise of values related to daily temperatures. The data is as follows:\n\n\nCode\n# Generating data\nweek_days &lt;- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\ndaily_temp &lt;- c(19, 19, 20, 19, 21, 20, 20)\n\nSamp_temp &lt;- data.frame(week_days, daily_temp)\n\n# Renaming columns\nnames(Samp_temp)[names(Samp_temp) == \"week_days\"] &lt;- \"Day\"\nnames(Samp_temp)[names(Samp_temp) == \"daily_temp\"] &lt;- \"Temperature\"\n\n# Plotting transposed (t) table\nknitr::kable(t(Samp_temp))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nSunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\nTemperature\n19\n19\n20\n19\n21\n20\n20\n\n\n\n\n\nBecause we’re cooped up indoors for the majority of the week, we might weight our opinions of weather of this week was ‘good’ or ‘bad’ based on the weekends. In this case, a weighted mean can be a helpful tool. It’s equation is as follows:\n\\[\n\\bar{x}_w = \\frac{\\sum_{i=1}^{n}(x_i \\cdot w_i)}{\\sum_{i=1}^{n}w_i} \\ \\ \\ \\ \\therefore \\ \\ \\ \\ \\bar{x}_w = \\frac{\\sum{wx}}{\\sum{w}}\n\\]\nHere, \\(w\\) represents your weights; as before, \\(x_i\\) are your indexed values. This formula may seem intimidating, however, all it is doing is allowing you to increase the importance of certain values over another by adjusting their multiplication factors. The arithmetic mean places equal weightings to observations so that the even weighting is \\(\\frac{1}{7} \\approx 0.142\\) for each day of the week; tallied up, their value will be \\(1\\):\n\n\nCode\n# Manually inputted original weightings (approximated)\nSamp_temp$original_weight &lt;- c(0.142, 0.142, 0.142, 0.142, 0.142, 0.142, 0.142)\n\n# Output table\nknitr::kable(t(Samp_temp[,c(1,3)]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nSunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\noriginal_weight\n0.142\n0.142\n0.142\n0.142\n0.142\n0.142\n0.142\n\n\n\n\n\nIf we wanted to place emphasis on Saturday and Sunday, we’d override the weights with higher decimal values placed on these days - ensuring that the total still comes up to \\(1\\):\n\n\nCode\n# Manually inputted original weightings (approximated)\nSamp_temp$adjusted_weight &lt;- c(0.1, 0.1, 0.1, 0.1, 0.1, 0.25, 0.25)\n\n# Output table\nknitr::kable(t(Samp_temp[,c(1,4)]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nSunday\nMonday\nTuesday\nWednesday\nThursday\nFriday\nSaturday\n\n\nadjusted_weight\n0.10\n0.10\n0.10\n0.10\n0.10\n0.25\n0.25\n\n\n\n\n\nWith these values, it is possible to calculate the weighted mean using the equation above:\n\\[\\begin{align}\n\\bar{x}_w & = \\frac{\\sum{(1.9, 1.9, 2, 1.9, 2.1, 5, 5)}} {(0.1, 0.1, 0.1, 0.1, 0.1, 0.25, 0.25)}\n\\\\\n\\\\\n& = \\frac{19.8 \\ \\cdot \\ 1}{1}\n\\\\\n\\\\\n& = 19.8\n\n\\end{align}\\] \\end{equation}\nIn R, the weighted.mean(...) function can be used calculate this value. Using our data frame Samp_temp the process is as follows:\n\n\nCode\nweighted.mean(x = Samp_temp$Temperature, w = Samp_temp$adjusted_weight)\n\n\n[1] 19.8\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible to generate rows of weighted means based on column values. However, this process requires the use of R’s apply(...) function in order to evaluate the array. The approximate format is as follows:\n\n\nCode\nyour_data$w_avg &lt;- apply(X = your_data,\n                         MARGIN = 1, \n                         FUN = function(x) weighted.mean(x = x[1:nrow(your_data), w = adjusted_column_weights))\n\n\n\n3.3.1 Description\nIn your data frame, create a new column that returns the weighted average. This new column will feature a weighted average per row of data. X = is specifies that you intend the column to fit into your data frame; MARGIN = 1 tells R that we want to calculate values for each row of data present; FUN = function(x) applies the weighted.mean(...) function to each data entry - the range being from the first entry to the last, denoted by nrow(your_data). For this last part, the x = and y = arguments are called in from the weighted.mean(...) function.\nRemember, its good practice to have your adjusted weights equal to exactly 1!.\n\n\n\n\n\n3.4 The mode\nEssentially, the mode (\\(M_o\\)) is easily grasped - it tells us the value that occurs most frequently (\\(f\\)), However, its formula can take some time to digest:\n\\[\\begin{equation}\nM_o = l + h\n\\begin{pmatrix}\n\\frac{f_m - f_1}{2f_m - f_1 - f_2}\n\\end{pmatrix}\n\\end{equation}\\]\nAlso, here is a vector showing the number of gears in the cars surveyed in the mtcars data set.\n\n\nCode\nmtcars$gear\n\n\n [1] 4 4 4 3 3 3 3 4 4 4 4 3 3 3 3 3 3 4 4 4 3 3 3 3 3 4 5 5 5 5 5 4\n\n\nIn the, \\(l\\) refers to the lower boundary of the modal class. Eyeballing the vector above, we can determine that this value is 3; we can also use the min(...) function to discover this. \\(h\\) refers to the size of the vectors we’re analysing (equivalent to \\(n\\)) - to find this, we can use the length(...) function in R; \\(f_m\\) refers to the frequency corresponding to a modal class (the number of 3s, 4s, and 5s); \\(f_1\\) is the preceding, to a modal class - for 3s this will be 0, for the 4s it will be number of 3s, etc; \\(f_2\\) is the number proceeding to the next modal class - for the 4s this will be the number of 5s.\nSolving the formula by hand is a lengthy process, so I won’t stress the details! Instead, it’s worth noting that R does not have a native function for this, you will need to build one from scratch(!); functions will be explained in more detail later in this website. For now let’s try the following based on code shared in Statology:\n\n\nCode\ncalc_mode &lt;- function(x) {\n  u &lt;- unique(x)\n  tab &lt;- tabulate(match(x, u))\n  u[tab == max(tab)]\n}\n\n\nThe key thing to note here is that only one piece of information is required here, your vector x. This function uses the unique(...) function that detects unique values and ignores duplications; tabulate(...) calculates the number of times a value appears against a unique entry - this is helped by the match(...) function. The returned result, is the maximum value (note the max(...) function) is the most frequently occurring value detected in the tabulation process.\nWhen the function is applied to the mtcars$gear vector, the result is as follows:\n\n\nCode\ncalc_mode(mtcars$gear)\n\n\n[1] 3\n\n\nThe importance of the mode is important for the next section as it can help to detect centrality."
  },
  {
    "objectID": "Descriptive_Statistics.html#visual-summaries",
    "href": "Descriptive_Statistics.html#visual-summaries",
    "title": "Descriptive Statistics",
    "section": "4 Visual summaries",
    "text": "4 Visual summaries\nUsing functions in R to calculate maximum, minimums, modes and means is useful in getting a ready to present statistics. However, there are times where examining the distribution of your data is advantageous. This can be achieved using three fundamental visualisations (charts):\n\nHistogram\nBox-Whisker Plot\nQ-Q plots\n\n\n4.1 Creating a Histogram in R\nA histogram works in a similar way to the mode, but doesn’t go as far as to give a result concerning the most frequently occurring number. It sorts data into ‘bins’.\n\n\n\n\nUsing the mtcars data set, we can organise the disp variable (engine displacement - cylinder volume) into 3 manually defined bins. The data is as follows:\n\n\nCode\nmtcars$disp\n\n\n [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0 146.7 140.8 167.6 167.6 275.8\n[13] 275.8 275.8 472.0 460.0 440.0  78.7  75.7  71.1 120.1 318.0 304.0 350.0\n[25] 400.0  79.0 120.3  95.1 351.0 145.0 301.0 121.0\n\n\n\n4.1.1 Bins\nThe bins are based on approximately equal breaks that are determined using R’s cut(...) function. A little leeway is given to the lowest value (rouding down to 70). The process of calculating the bins in R is as follows:\n\n\nCode\nrange(mtcars$disp)\n\n\n[1]  71.1 472.0\n\n\nCode\nbin &lt;- cut(mtcars$disp, 3, dig.lab = 5)\nunique(bin)\n\n\n[1] (70.699,204.73] (204.73,338.37] (338.37,472.4] \nLevels: (70.699,204.73] (204.73,338.37] (338.37,472.4]\n\n\nCode\nbin_2 &lt;- c(70, 204, 338)\n\ntable(cut(mtcars$disp,c(bin_2,Inf), dig.lab = 3))\n\n\n\n (70,204] (204,338] (338,Inf] \n       16         8         8 \n\n\n\n\n4.1.2 Base plotting\nThis data can be easily presented as a histogram using R’s hist(...) function. The prob = FALSE argument ensures that we are only examining frequencies (and not probabilistic density - discussed shortly). Other arugments include:\n\nlabels. When set to TRUE, this argument generates a numerical value that floats above the bars (representing the bin values). These have been included for illustrative purposes and are not always needed in most situations.\nylim. This argument sets the limit for the plot’s y-axis. Here, it has been extended so that the labels are clearly visible.\nxlab. This argument allows you to override the default x-axis text with something more suitable.\nmain. This argument allows you to change the title of the plot. As this is a string value, it is encased in quotation marks.\ncol. This argument sets the colour of the plot. This can be a value from R’s default colour palettes or given as a hex value. See this cheatsheet for more information.\n\n\n\nCode\nhist(x = mtcars$disp, breaks = c(70, 204, 338, 500),\n     labels = TRUE,\n     prob = FALSE,\n     ylim = c(0,20),\n     xlab = \"Displacement (cu.in.)\",\n     main = \"Histogram Example 1\",\n     col = \"palegreen\")\n\n\n\n\n\nA histogram can be defined by the number of bins it possess. In a base plot, this is done using n =:\n\n\nCode\nhist(x = mtcars$disp, breaks = 2,\n     labels = TRUE,\n     prob = FALSE,\n     ylim = c(0,20),\n     xlab = \"Displacement (cu.in.)\",\n     main = \"Histogram Example 1\",\n     col = \"palegreen\")"
  },
  {
    "objectID": "Descriptive_Statistics.html#box-whisker-plots",
    "href": "Descriptive_Statistics.html#box-whisker-plots",
    "title": "Descriptive Statistics",
    "section": "5 Box-whisker plots",
    "text": "5 Box-whisker plots\nAccording to Tukey, a Box-and-whisker plot is “a box from hinge to hinge, bared at the median, with”whiskers” to (a) extremes (b) the innermost identified values (c) the adjacent values” (1977). A hinge is defined as \\(\\frac{1}{2}(1+m)\\), where \\(m\\) indicates the depth of the median - it is a quartile. Remember, when applied to a variable, the summary(...) function returns the first and third quartile values:\n\n\nCode\nsummary(mtcars$disp)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   71.1   120.8   196.3   230.7   326.0   472.0 \n\n\nBox plot\n\n\n\n\n\n\n\n\n\nA way to see these in context is by visualising them. An easy way to do this is with a Box-whisker plot. In R, this is easily invoked using the boxplot(...) function on the same displacement data featured above:\n\n\nCode\nboxplot(mtcars$disp, horizontal = T, col = \"palegreen\")"
  },
  {
    "objectID": "Descriptive_Statistics.html#box-and-whisker-plots",
    "href": "Descriptive_Statistics.html#box-and-whisker-plots",
    "title": "Descriptive Statistics",
    "section": "5 Box-and-whisker plots",
    "text": "5 Box-and-whisker plots\nAccording to Tukey, a Box-and-whisker plot is “a box from hinge to hinge, bared at the median, with”whiskers” to (a) extremes (b) the innermost identified values (c) the adjacent values” (1977). A hinge is defined as \\(\\frac{1}{2}(1+m)\\), where \\(m\\) indicates the depth of the median - it is a quartile. Remember, when applied to a variable, the summary(...) function returns the first and third quartile values:\n\n\nCode\nsummary(mtcars$disp)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   71.1   120.8   196.3   230.7   326.0   472.0 \n\n\nBox-and-whisker plots are commonly referred to as box plots and can be considered a visual representation of R’s summary(...) output; from the example showing in Figure 1, at the expense of precision, it can be determined that the median of the data is approximately 196 and that the majority of observations have engine displacements between approximately 121 and slightly above 321 cubic inches.\n\n\n\n\n\nFigure 1: An example of a single box plot (created using ggplot2)\n\n\n\n\nIn R, this is easily invoked using the boxplot(...) function on the same displacement data featured above (see Figure 2). Again, additional arguments are applied to enhance the aesthetics of the plot:\n\ncol.main allows for the colour of the title to be changed.\nxaxt switches off the x-axis\n\nFurthermore, a new function is introduced, axis(...). This function reintroduces the x-axis but with customisable number of ticks. An explanation of its arguments is as follows:\n\nside specifies the axis that we want to alter. 1 refers to the x-axis, while 2 refers to the y-axis. In this plot, the x-axis is where changes are being made\nat specifies where we want to draw our ticks. Here, I have concatenated a sequence of ticks every 50 units between a range of 0 and 600 using the seq(...) function within.\nlas alters the orientation of the axis text. Here, 2 is used to orientate the text so that it is perpendicular to the axis. A value of 1, forces a horizontal orientation, and 3 forces a vertical orientation. 0 is the default orientation - parallel to the axis.\n\n\n\nCode\nboxplot(x = mtcars$disp, horizontal = T, col = \"palegreen\",\n        main = \"Engine Displacement (cu.in.)\",\n        col.main = \"red\",\n        xaxt = \"n\")\naxis(side = 1, at = c(seq(0,600,50)), las = 2)\n\n\n\n\n\nFigure 2: box plot created using base R\n\n\n\n\n\n5.1 Multiple box-plots\nObserving a single box plot yields a significant amount of information. However, its application becomes significantly richer when examining grouped (banded) data.\nFigure 3 shows engine displacement against the number of gears from the mtcars data set featured so far. To achieve this effect the arugment formula = is used instead of x =; subsequently, a formulaic expression is used y ~ x - disp is actually the y-axis and gear, the x-axis. The order is flipped when horizontal = TRUE is evoked.\n\n\nCode\nboxplot(formula = disp ~ gear, data = mtcars, horizontal = TRUE,\n        xaxt = \"n\",\n        xlab = \"Displacement (cu.in.)\",\n        ylab = \"Number of gears\",\n        col = \"palegreen\")\naxis(side = 1, at = c(seq(0,600,50)), las = 2)\n\n\n\n\n\nFigure 3: Multiple box-whisker plots\n\n\n\n\nFrom this chart the assertion can be made that a portion of cars with 4 or 5 gear transmissions have similar displacements, particularly at smaller volumes of around 150 cubic inches or less - the middle 50% (interquartile ranges) of the observations intersect.\nThe left-whisker of the 3 gear transmission cars, indicates that there are observations that fall below the interquartile range, within 1.5 multiplications - there are number of cars with 3 gears that share the upper end of 4-gear transmission cars. However, the bulk of its engine displacement records are shared with 5-gear transmission cars. Comparatively, 3-gear transmission cars have higher engine displacement.\nComparing groups of observations is covered in more detail and with more precision in the ‘inferential statistics’ part of this website."
  },
  {
    "objectID": "Descriptive_Statistics.html#other-useful-visualisations",
    "href": "Descriptive_Statistics.html#other-useful-visualisations",
    "title": "Descriptive Statistics",
    "section": "6 Other useful visualisations",
    "text": "6 Other useful visualisations\nDuring your previous studies you may have come across pie charts, bar plots, and scatters. These plots are useful in aiding in the description of statistics can be built in R.\n\n6.1 Pie charts\nPie charts came about towards the end of 18th century. They show proportions of observations and may be accompanied with annotations such as percentages or counts, and may include a legend.\n\n6.1.1 Data preparation\nTo construct a pie chart in R, you’ll need to supply discrete values. Here, we can use the aggregate(...) function in R to calculate the total number of observations for a predetermined category. Let’s consider the number of gears for straight-engine cars from the mtcars data set. and present is as a frequency table.\nThe dimensions of the frequency table are defined by a formula that follows an understanding that \\(y = f(x)\\). In other words, changes along the y-axis are determined by changes along the x-axis; \\(y\\) is dependent on \\(x\\) - this is the premise of linear regression that features later in this manual. Our premise is that shape of the engine determines the number of gears in a car. Therefore, \\(y\\) is the vs variable, while x is the number of gears. As we’re interested in the maximum number of gears the FUN = argument is set to the R function sum (without parentheses). The code is as follows:\n\n\nCode\nfreq_data &lt;- aggregate(x = vs~gear, data = mtcars, FUN = sum)\n\n\nAs we are keen to work out the proportion as a percentage, we create a new column in the data frame. The easiest way to this is to add a dollar sign ($) after the name of the data frame and follow this with an appropriate column name i.e. data$name. To calculate a percentage the formula is \\(\\frac{x}{\\sum{x}} \\times 100\\); a discrete value divided by the row total and then multiplied by 100.\nThis process produces a continuous value with number of decimal places (d.p.). To control for this, the round(...) function may be used. In principle, this function is round(x, digits = n) - to round 12.18 to 1 d.p., the function will be as follows round(x = 12.18, digits = 1).\nThe process of producing a column in our data where a percentage is calculated to 1 d.p. is as follows:\n\n\nCode\nfreq_data$pct &lt;- round(x = freq_data$vs/sum(freq_data$vs)*100,1)\n\n\n\n\n6.1.2 Generating the graphic\nConstructing a pie chart in R is achieved using the pie(...) function. The function can be as simple as that used to generate Figure 4.\n\n\nCode\npie(x = freq_data$vs)\n\n\n\n\n\nFigure 4: A simple pie chart\n\n\n\n\nHowever, this output doesn’t tell us to too much beyond the proportionality of something. To address this, we can augment the function with labels based on the percentages we’ve already worked out:\n\n\nCode\npie(x = freq_data$vs, labels = freq_data$pct)\n\n\n\n\n\nFigure 5: A simple pie chart with proportional labels\n\n\n\n\nWhile this graphic is fine, it will be better to have actual percentage signs shown. This can be achieved by using the paste(...) function within the labels = argument. This function allows for the insertion of additional text of characters to help enrich an output. For example:\n\n\nCode\nrandom_value = 12.6\nprint(random_value)\n\n\n[1] 12.6\n\n\nCode\n# To add a % sign...\nprint(paste(random_value,\"%\", \"\"))\n\n\n[1] \"12.6 % \"\n\n\nThere is a space between the number and the percentage sign. To remove this, you can use the sep = \"\" argument within the paste(...) function.\n\n\nCode\nprint(paste(random_value, \"%\", sep = \"\"))\n\n\n[1] \"12.6%\"\n\n\nApplied to our data:\n\n\nCode\npie(x = freq_data$vs, labels = paste(freq_data$pct,\"%\",sep = \"\"))\n\n\n\n\n\nFigure 6: A simple pie chart with clearly annotated proportions\n\n\n\n\nFinal steps concern aesthetics. If you wanted to specify the colouts yourself, you can do this by creating a new vector that contains the colour codes that you are interested in. For example: custom_col &lt;- c(\"salmon\",\"seagreen\",\"palegreen\"). When plotting your pie chart, you can activate this palette by using the col = argument (see Figure 7)\n\n\nCode\ncustom_col &lt;- c(\"salmon\",\"seagreen\",\"palegreen\")\n\npie(x = freq_data$vs, labels = paste(freq_data$pct,\"%\",sep = \"\"), col = custom_col)\n\n\n\n\n\nFigure 7: A pie chart with proportions and customised colour palette\n\n\n\n\nAs we have no idea of telling what the colours relate to (in terms of car gears), we add a legend based on the number of gears. This is achieved using the legend(...) function on a new line of code. This line of code is dependent on a plot being produced beforehand and cannot be run in isolation. The x = argument in the function specifies the position of the legend, while the legend = argument specifies the labs for your legend; fill = will be the same colour palette used in the pie chart, and title = is the title for your legend. For example:\n\n\nCode\ncustom_col &lt;- c(\"salmon\",\"seagreen\",\"palegreen\")\n\npie(x = freq_data$vs, labels = paste(freq_data$pct,\"%\",sep = \"\"),\n    col = custom_col)\nlegend(x = \"topleft\", legend = c(freq_data$gear), fill = custom_col, title = \"No. gears\")\n\n\n\n\n\nFigure 8: A final render of a pie chart (including legend)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen using a custom colour scale with discrete values, ensure that you have the name number of colours as discrete values.\n\n\n\n\n\n6.2 Bar plots\nBar plots (or bar graphs) are visual representations of discrete data frequencies; its height is relative to the number of times a value appears in a vector/variable (see Harris and Jarvis 2014).\nCreating bar plots in R works in a similar way to the pie chart. Here, the barplot(...) function is used to produce the base graphic, where height = indicates the variable that determines the height of the bars - in the example shown in Figure 5, the variable (vs) is equal to 1, representing the number of straight engine cars.\nA simple barplot(...) for our data will be as follows:\n\n\nCode\nbarplot(height = freq_data$vs)\n\n\n\n\n\nFigure 9: A bar plot\n\n\n\n\nTo furnish this with details, you can use the col = argument to introduce a customised colour scale, use the names.arg = to name the respective columns, and use the las = 2 argument to flip the orientation of the labels. Defining the barplot as an object called bp returns both the object and plot.\n\n\nCode\nbp &lt;- barplot(height = freq_data$vs, col = custom_col, names.arg = paste(\"Gears:\",freq_data$gear), las = 2)\n\n\n\n\n\nFigure 10: A labelled bar plot\n\n\n\n\nThe bp object can be used in the text(...) function to create labels that float above the bars. Here, we want the text to be bp values, and the labels to be located at at a good distance above the maximum height of each bar (0.5 units).\nAgain, the legend function is used to produce a legend. This time, it is located to the top-right of the plot space.\n\n\nCode\nbp &lt;- barplot(height = freq_data$vs, col = custom_col, names.arg = paste(\"Gears:\",freq_data$gear), las = 2, ylim = c(0, max(freq_data$vs + 1)))\n\ntext(x = bp, y = freq_data$vs + 0.5, labels = freq_data$vs)\n\nlegend(x = \"topright\", legend = c(freq_data$vs), fill = custom_col, title = \"No. Straight Engines\")\n\n\n\n\n\nFigure 11: A bar plot with labels and legend"
  },
  {
    "objectID": "Descriptive_Statistics.html#sec-GDF",
    "href": "Descriptive_Statistics.html#sec-GDF",
    "title": "Descriptive Statistics",
    "section": "2 Geographic Data Frames (GDF)",
    "text": "2 Geographic Data Frames (GDF)\nA data frame is a the next step up from a matrix. While a matrix contains a single class of data (i.e. purely numeric), a data frame can contain difference classes of data variable. An example of a data frame is noted below along with a test of the object’s class. The data shows the effect of vitamin C on the growth rate of teeth for the first 6 rows of 60 observations.\n\n\nCode\nhead(x = ToothGrowth, n = 10)\n\n\n    len supp dose\n1   4.2   VC  0.5\n2  11.5   VC  0.5\n3   7.3   VC  0.5\n4   5.8   VC  0.5\n5   6.4   VC  0.5\n6  10.0   VC  0.5\n7  11.2   VC  0.5\n8  11.2   VC  0.5\n9   5.2   VC  0.5\n10  7.0   VC  0.5\n\n\nCode\nclass(x = ToothGrowth)\n\n\n[1] \"data.frame\"\n\n\nA geographic data frame differs from a conventional data frame as it includes geographic data such as geometries, longitude, and latitude. Geographic data fields, make it possible to spatialise and map your data.\n\n\n\n\n\n\nClass in data objects\n\n\n\nClass is a shortened handle for classification. R is able to identify a range of different classes either via the summary(...) or class(...) functions. Knowing the class of your data will help you to select the most appropriate instruments for analysis."
  },
  {
    "objectID": "Descriptive_Statistics.html#spatialised-data",
    "href": "Descriptive_Statistics.html#spatialised-data",
    "title": "Descriptive Statistics",
    "section": "7 Spatialised data",
    "text": "7 Spatialised data\nSo far, we have largely focused on the non-geographic data set, mtcars. As someone engaged with a geographic discipline, it is highly likely that you will be engaging geographical data frames (see Section 2). Such data sets have the advantage of being able to produce cartographic representations - maps!\nPrincipally, R is a statistical script-based programming language without mapping capabilities. To produce maps in R you’ll need to read in a package called sf. sf is an abbreviation of Simple Features (pebesma2023?) that handles a data-frame objects with simplified code-lines including geographic formats.\n\n7.1 Shapefiles\nGeographic data come in variety of formats but essentially categories as either being vector-based or rasterised. For now, we’ll focus on manipulating a vector data frame containing polygons. First, to load sf if you’ll need to install package and then load its libraries into your R session. A way doing this with minimal as is as follows:\n\n\nCode\nif(!require(sf)) install.packages(\"sf\")\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen running a line of code similar that above, in a script, you may need to run it twice for the package to install and then to load. To avoid this, you can also use the following approaches:\n\n\nCode\ninstall.packages(\"sf\")\nrequire(sf)\n\n\nIf you have already have the package installed you may want to comment-out the first line by placing a hashtag before it:\n\n\nCode\n#install.packages(\"sf\")\nrequire(sf)"
  },
  {
    "objectID": "Mapping_1.html",
    "href": "Mapping_1.html",
    "title": "Choropleth mapping",
    "section": "",
    "text": "0.1 Introduction\nSo far, we have largely focused on the non-geographic data set, mtcars. As someone engaged with a geographic discipline, it is highly likely that you will be engaging geographical data frames. Such data sets have the advantage of being able to produce cartographic representations - maps!\nPrincipally, R is a statistical script-based programming language without mapping capabilities. To produce maps in R you’ll need to read in a package called sf. sf is an abbreviation of Simple Features (Pebesma and Bivand 2023) that handles a data-frame objects with simplified code-lines including geographic formats.\nMaps are initially complex to make in R but offer another way of describing data with geographic properties.\n\n\n0.2 Shapefile basics\nGeographic data come in variety of formats but essentially categories as either being vector-based or rasterised. For now, we’ll focus on manipulating a vector data frame containing polygons. First, to load sf if you’ll need to install package and then load its libraries into your R session. A way doing this with minimal as is as follows:\n\n\nCode\nif(!require(sf)) install.packages(\"sf\")\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen running a line of code similar that above, in a script, you may need to run it twice for the package to install and then to load. To avoid this, you can also use the following approaches:\n\n\nCode\ninstall.packages(\"sf\")\nrequire(sf)\n\n\nIf you have already have the package installed you may want to comment-out the first line by placing a hashtag before it:\n\n\nCode\n#install.packages(\"sf\")\nrequire(sf)\n\n\n\n\nAn example of geographic data can Tenure types recorded in Bristol, at the Lower Super Output Area (LSOA) level: https://raw.githubusercontent.com/Richtea84/I2Q-files/main/Bristol_Tenure.csv\n\n0.2.1 Loading the CSV into R\nIt is possible to load the CSV into R as an object using the weblink above in using the read.csv(...) function. Let’s call the Object ‘Bristol_data’.\n\n\nCode\nBristol_data &lt;- read.csv(file = \"https://raw.githubusercontent.com/Richtea84/I2Q-files/main/Bristol_Tenure.csv\")\n\n\nYou can preview the results using the head(...) function. For example:\n\n\nCode\nhead(x = Bristol_data, n = 5)\n\n\n  X2021.super.output.area...lower.layer Total..All.households Owned\n1              E01014601 : Bristol 001A                   746   474\n2              E01014602 : Bristol 001B                   827   353\n3              E01014603 : Bristol 001C                  1068   604\n4              E01014605 : Bristol 001E                   835   237\n5              E01032516 : Bristol 001G                   859   578\n  Shared.ownership Social.rented Private.rented Lives.rent.free\n1                5           187             77               3\n2                6           373             95               0\n3               27           195            242               0\n4                1           473            120               4\n5                7           139            132               3\n  Owns.with.a.mortgage.or.loan.or.shared.ownership\n1                                              247\n2                                              196\n3                                              371\n4                                              137\n5                                              252\n  Private.rented.or.lives.rent.free\n1                                80\n2                                95\n3                               242\n4                               124\n5                               135\n\n\nAs discussed in previous sections, there are ways in which you can change the column names - for now, we’ll leave them as they are. The data we have, is a ‘data frame’.\n\n\n0.2.2 Loading Mappable data (shapefile)\nAs we are looking at 2021 LSOAs for Bristol, we can access the corresponding geographic boundaries from UK data service: https://borders.ukdataservice.ac.uk/\nOnce there do the following:\n\nSelect the ‘Boundary Data Selector’ tool\nIn the first ‘select’ Box choose england, and in the third, select ‘2011 and later’. Click on the ‘Find’ button.\nIn the ‘Boundaries’ box select ‘English Lower Layer Super Output Areas, 2021’.\nList the the Areas. As of writing this page, only ‘England’ is available - this brings in the whole of the UK’s Lower Super Output Areas.\nEnsure that the data format is set to SHAPE with an archive format of Zip and click on ‘Extract Boundary Data’.\n\n\n\n\n\n\nUK Data Service Census Support’s Boundary Data Selector with relevant settings.\n\n\n\n\nYou may encounter further prompts related to the selection of the data download format - ensure you pick the ‘Download features in Shapefile format as Zip file’ for your selected data set when prompted.\nAt this point, it is important that a relevant working directory has been set up. The directory will contain both you CSV and the zipped data that you’ve downloaded. Next, boundary data is unzipped and subsequently loaded into R as a new object (called Bristol_LSOAs) using the read_sf(...) function from the sf package:\n\n\nCode\nBristol_LSOAs &lt;- read_sf(dsn = \"England_lsoa_2021/england_lsoa_2021.shp\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the shapefile (.shp) to load correctly, ensure that all of the other dependencies associated with the shapefile are present. Here, these are:\n\ndbf (data base)\nprj (projection)\nshx (geometries)\n\nUnder some circumstances, other dependencies may be present such as:\n\ncpg (code page for character sets)\nsbn (spatial indexing essential for processing the final output)\nsbx (furhter spatial indexing information used in rendering the final output)\n\nWhatever comes across in the download, ensure that it is present in the folder from which you are reading the .shp file.\n\n\n\n\n0.2.3 Joining the CSV data to the Mappable data\nThe process of joining CSV data to the mappable data uses the merge(...) function in exactly the same way as before. An important detail is that the mappable data is always the first object to be called in.\nFirst, let’s change the name of the field containing the containing the LSOA code reference (beginning ‘E01’).\n\n\nCode\n# Changing the name of the 'code field'\nnames(Bristol_data)[1] = \"code\"\n\n\nThe ‘code’ field we’ve just created contains too much information, making it impossible to match to Bristol_LSOAs data. The LSOA code is found in the first 9 characters. To shorten the contents here, we can use the substr(...) function:\n\nThe field we want to shorten forms the front of our argument\nx = is the same field\nstart = specifies the start of the crop - here, it is the first character\nstop = specifies the end of the crop - here, it is the ninth character\n\n\n\nCode\n# Shortening the field to the first 9 letters\nBristol_data$code &lt;- substr(x = Bristol_data$code, start = 1, stop = 9)\n\n\nWith the fields properly prepared, we are able to conduct the join using the merge(...) function. Again, note that the geographic data comes first (x=).\n\n\nCode\n# Performing the merge\nBristol_merged &lt;- merge(x = Bristol_LSOAs, y = Bristol_data, by.x = \"lsoa21cd\", by.y = \"code\")\n\n\n\n\n0.2.4 Previewing\nWith sf loaded, it is possible to preview the result of the join using the plot function. Let’s examine the first 6 plots using a slicing:\n\n\nCode\nplot(Bristol_merged[,c(1:6)])\n\n\n\n\n\nFigure 1: Preview of merged shapefile data\n\n\n\n\n\n\n\n0.3 Clipping and cropping\nBoundary of the Bristol featured in the LSOA data features the Avonmouth Extension (Figure 1) that protudes out into the sea due the region’s in relation to Bristol’s maritime history that saw shipping trade attributed to the large tidal range of the River Avon that enabled Bristol’s Floating Harbour; shipping traffic falls under the jurisdiction of Bristol’s port authority within the extension. However, we are only interested with the land administration component, where we will need to clip the region accordingly.\n\n0.3.1 Dissolving\nThe first step is to ‘clip’ the extension off so that we are only looking at the land. This is achieved by downloading the boundaries for Bristol’s wards. The Shapefile version of the data should be downloaded and loaded following a similar process to that described in Section 0.2.2.\nLet’s load the shapefile as an object called ‘Bristol_wards’ (ensuring all of the dependencies are present):\n\n\nCode\nBristol_wards &lt;- read_sf(\"Wards/Wards.shp\")\n\n\nWe can plot one of the variables using the plot(...) function:\n\n\nCode\nplot(Bristol_wards[\"NAME\"])\n\n\n\n\n\nFigure 2: Bristol Wardss layer (NAME field sliced)\n\n\n\n\nThis good, but let’s merge all of the separate polygons into a single landmass. The sf package features a useful function that dissolves boundaries called st_union(...):\n\n\nCode\nBristol_Mass &lt;- st_union(x = Bristol_wards)\nplot(Bristol_Mass, col = \"palegreen\")\n\n\n\n\n\nFigure 3: The dissolved wards layer\n\n\n\n\n\n\n0.3.2 Cropping\nNow that we have the dissolved (or unionised) layer, we are in a position to clip off the Avonmouth Extension. Here’s we’ll use the st_clip(...) function and use the Bristol_Mass object as the mask.\n\nx = The data that needs to be cropped (Bristol_merged)\ny = The cropping background\n\n\n\nCode\nBristol_merge_2 &lt;- st_crop(x = Bristol_merged, y = Bristol_Mass)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nCode\nplot(Bristol_merge_2[\"Social.rented\"])\n\n\n\n\n\nA preview of Bristol’s social rentals (2021) excluding the Avonmouth Extension\n\n\n\n\nAt this point, we are examining continuous data where the blue tones indicate lower observation frequencies, while the yellow tones are the higher frequencies.\n\n\n\n0.4 tmap cartography\nInspecting social rentals, we can detect areas where social rentals (i.e. council tenancies) are highest - in the middle and towards the bottom. However, it is difficult to ascertain scale and location, where this graphic lacks cartographic elements and cannot be considered a ‘map’ at this stage. To introduce cartographic elements another package needs to be introduced, tmap (Tennekes 2018). Loading this package follows a similar process to that described above:\n\n\nCode\nif(!require(tmap)) install.packages(\"tmap\")\n\n\nLoading required package: tmap\n\n\nThe parsing for tmap differs from the default R coding structure and closely aligned to an extended graphical suite called ggplot that features in other programming languages such as Python. Here’s the code for producing a basic preview for our clipped data with tmap is as follows:\n\n\nCode\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\")\n\n\n\n\n\nFigure 4: Preview of social rents in Bristol using tmap\n\n\n\n\n\ntm_shape(...) introduces the geographic (mappable data)\ntm_polygons(...) specifies the geometry and its aesthetic properties.\n\nThese are linked together with + symbols.\nTo make this graphic cartographic, we need to include the following elements:\n\nCompass: function is tm_compass(...)\nScale bar: function is tm_scale_bar(...)\nGraticules (longitude and latitude lines): function is tm_graticules(...)\n\nAdding these on can be achieved by using further + symbols in your code. An example is shown in Figure 5.\n\n\nCode\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\")+\n  tm_compass()+\n  tm_scale_bar()+\n  tm_graticules()\n\n\n\n\n\nFigure 5: A map (with cartographic features) produced in tmap\n\n\n\n\n\n0.4.1 Aesthetic manipulation\nAs with the plot(...) function, colours and elements can be manipulated. You can learn more about how to customise the look and feel of your map by inserting ? in front of the graphical element that you want to manipulate.\nAn example of how arguments can be applied to improve the map above is as follows:\n\n\nCode\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"navy\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"navy\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\n\n\n\nFigure 6: Various aesthetic manipulations applied to out data\n\n\n\n\nIn addition to the placement, the map’s default colour scheme can also be manipulated. Using either a default Brewer palette, importing a palette library, or by creating your own customised palette. Here’s an example of a map produced using a custom colour palette (Figure 7)\n\n\nCode\ncustom_pal &lt;- c(\"red\",\"grey20\")\n\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0, palette = rev(custom_pal), n = 12)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"white\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"white\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\n\n\n\nFigure 7: Application of a custom colour palette\n\n\n\n\nA critical step in the production of this graphic is the use of palette = argument that points to the custom colour palette. Within this argument the rev(...) function is applied to reverse the colour palette.\n\n\n\n0.5 Basemaps\nWhile someone with a good understanding of geometry can figure out the precise location of the featured regions, for most the inclusion of a base map is far more practical. This can be added using a package called rosm (Dunnington 2022) that has access to Open Street Map map tiles stored in a raster format. We’ll discuss rasters in another section.\n\n\nCode\n# Installing rosm\nif(!require(rosm)) install.packages(\"rosm\")\n\n\nLoading required package: rosm\n\n\n\n0.5.1 Changing map projection\nAs Open Street Map is projected to the Mercator projection (same as Google Maps), we need to change the projection of our map data (Bristol_merge_2) so that it conforms. The EPSG code we need is 4326 This can be achieved using the st_transform(...) from the sf package. Here is how it is applied in the creation of a new object simply called Bristol_reproj:\n\n\nCode\nBristol_reproj &lt;- st_transform(x = Bristol_merge_2, crs = 4326)\n\n\nWith the application of the Mercator projection assigned to our data, we can now retrieve a relevant base map. For this, we need the osm.raster(...) function from rosm and the st_bbox(...) function from sf to define the bounding box (or rectangular extents) of our data.\n\n\nCode\nmy_basemap &lt;- rosm::osm.raster(st_bbox(Bristol_reproj),)\n\n\nZoom: 12\n\n\nTo add the base map, a raster image, we use tm_shape(...) in combination with the tm_rgb(...) function from tmap. Here’s our base map:\n\n\nCode\ntm_shape(my_basemap)+\n  tm_rgb()\n\n\n\n\n\nFigure 8: The retreived rosm basemap\n\n\n\n\nAs with other elements in tmap the basemap can be manipulated. Let’s make it monochrome by lowering the saturation = argument to 0:\n\n\nCode\ntm_shape(my_basemap)+\n  tm_rgb(saturation = 0)\n\n\n\n\n\nFigure 9: The base map in monochrome, saturation set to 0\n\n\n\n\nAdding our mapped data onto of this is this base map is achieved by adding the map code immediately after the base map. Transparency on for out polygon layer is achieved by introducing the alpha = argument, the selected value is 0.45:\n\n\nCode\ntm_shape(my_basemap)+\n  tm_rgb(saturation = 0)+\n  tm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0, palette = rev(custom_pal), n = 12, alpha = 0.45)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"white\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"white\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\n\n\n\nFigure 10: Applying the base map to our mapped data…\n\n\n\n\nAs seen in Figure 10, we are able to contextualise data we’ve imported by seeing the names of nearby areas. Looking at our web browswers it is possible to identify that Hartcliffe and areas close to Easton, Redfield, and St Philips are have high occurrences of social rented housing.\n\n\n\n0.6 Interactive maps\nA helpful feature in tmap is its ability to produce interactive web maps. These are essentially rendered using a javascript package known as leaftlet. Creating an interactive map in tmap is achieved by using the following command before your main code tmap_mode(\"view\"). Here’s the result when applied to our map above:\n\n\nCode\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nCode\n#tm_shape(my_basemap)+\n  #tm_rgb(saturation = 0)+\n  tm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0, palette = rev(custom_pal), n = 12, alpha = 0.45)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"white\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"white\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\nCompass not supported in view mode.\n\n\nlegend.postion is used for plot mode. Use view.legend.position in tm_view to set the legend position in view mode.\n\n\nWarning: In view mode, scale bar breaks are ignored.\n\n\n\n\n\n\nFigure 11: An example of an interactive map\n\n\n\nA crucial concern with this mode of mapping is the loss of cartographic elements including the graticules, and compass - this results in a number of warnings when our raw code is introduced. Since several base maps are supplied by default, we no longer have to generate a base map. To change the mode back, we use the following line of code:\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\n\n\n\n\n\nReferences\n\nDunnington, Dewey. 2022. Rosm: Plot Raster Map Tiles from Open Street Map and Other Sources. https://CRAN.R-project.org/package=rosm.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With applications in R. Chapman and Hall/CRC. https://r-spatial.org/book/.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06."
  },
  {
    "objectID": "Mapping_1.html#vector-mapping",
    "href": "Mapping_1.html#vector-mapping",
    "title": "Choropleth mapping",
    "section": "",
    "text": "So far, we have largely focused on the non-geographic data set, mtcars. As someone engaged with a geographic discipline, it is highly likely that you will be engaging geographical data frames. Such data sets have the advantage of being able to produce cartographic representations - maps!\nPrincipally, R is a statistical script-based programming language without mapping capabilities. To produce maps in R you’ll need to read in a package called sf. sf is an abbreviation of Simple Features (Pebesma and Bivand 2023) that handles a data-frame objects with simplified code-lines including geographic formats.\nMaps are initially complex to make in R but offer another way of describing data with geographic properties.\n\n\nGeographic data come in variety of formats but essentially categories as either being vector-based or rasterised. For now, we’ll focus on manipulating a vector data frame containing polygons. First, to load sf if you’ll need to install package and then load its libraries into your R session. A way doing this with minimal as is as follows:\n\n\nCode\nif(!require(sf)) install.packages(\"sf\")\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen running a line of code similar that above, in a script, you may need to run it twice for the package to install and then to load. To avoid this, you can also use the following approaches:\n\n\nCode\ninstall.packages(\"sf\")\nrequire(sf)\n\n\nIf you have already have the package installed you may want to comment-out the first line by placing a hashtag before it:\n\n\nCode\n#install.packages(\"sf\")\nrequire(sf)\n\n\n\n\nAn example of geographic data can Tenure types recorded in Bristol, at the Lower Super Output Area (LSOA) level: https://raw.githubusercontent.com/Richtea84/I2Q-files/main/Bristol_Tenure.csv\n\n\nIt is possible to load the CSV into R as an object using the weblink above in using the read.csv(...) function. Let’s call the Object ‘Bristol_data’.\n\n\nCode\nBristol_data &lt;- read.csv(file = \"https://raw.githubusercontent.com/Richtea84/I2Q-files/main/Bristol_Tenure.csv\")\n\n\nYou can preview the results using the head(...) function. For example:\n\n\nCode\nhead(x = Bristol_data, n = 5)\n\n\n  X2021.super.output.area...lower.layer Total..All.households Owned\n1              E01014601 : Bristol 001A                   746   474\n2              E01014602 : Bristol 001B                   827   353\n3              E01014603 : Bristol 001C                  1068   604\n4              E01014605 : Bristol 001E                   835   237\n5              E01032516 : Bristol 001G                   859   578\n  Shared.ownership Social.rented Private.rented Lives.rent.free\n1                5           187             77               3\n2                6           373             95               0\n3               27           195            242               0\n4                1           473            120               4\n5                7           139            132               3\n  Owns.with.a.mortgage.or.loan.or.shared.ownership\n1                                              247\n2                                              196\n3                                              371\n4                                              137\n5                                              252\n  Private.rented.or.lives.rent.free\n1                                80\n2                                95\n3                               242\n4                               124\n5                               135\n\n\nAs discussed in previous sections, there are ways in which you can change the column names - for now, we’ll leave them as they are. The data we have, is a ‘data frame’.\n\n\n\nAs we are looking at 2021 LSOAs for Bristol, we can access the corresponding geographic boundaries from UK data service: https://borders.ukdataservice.ac.uk/\nOnce there do the following:\n\nSelect the ‘Boundary Data Selector’ tool\nIn the first ‘select’ Box choose england, and in the third, select ‘2011 and later’. Click on the ‘Find’ button.\nIn the ‘Boundaries’ box select ‘English Lower Layer Super Output Areas, 2021’.\nList the the Areas. As of writing this page, only ‘England’ is available - this brings in the whole of the UK’s Lower Super Output Areas.\nEnsure that the data format is set to SHAPE with an archive format of Zip and click on ‘Extract Boundary Data’.\n\n\n\n\n\n\nUK Data Service Census Support’s Boundary Data Selector with relevant settings.\n\n\n\n\nYou may encounter further prompts related to the selection of the data download format - ensure you pick the ‘Download features in Shapefile format as Zip file’ for your selected data set when prompted.\nAt this point, it is important that a relevant working directory has been set up. The directory will contain both you CSV and the zipped data that you’ve downloaded. Next, boundary data is unzipped and subsequently loaded into R as a new object (called Bristol_LSOAs) using the read_sf(...) function from the sf package:\n\n\nCode\nBristol_LSOAs &lt;- read_sf(dsn = \"England_lsoa_2021/england_lsoa_2021.shp\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the shapefile (.shp) to load correctly, ensure that all of the other dependencies associated with the shapefile are present. Here, these are:\n\ndbf (data base)\nprj (projection)\nshx (geometries)\n\nUnder some circumstances, other dependencies may be present such as:\n\ncpg (code page for character sets)\nsbn (spatial indexing essential for processing the final output)\nsbx (furhter spatial indexing information used in rendering the final output)\n\nWhatever comes across in the download, ensure that it is present in the folder from which you are reading the .shp file.\n\n\n\n\n\nThe process of joining CSV data to the mappable data uses the merge(...) function in exactly the same way as before. An important detail is that the mappable data is always the first object to be called in.\nFirst, let’s change the name of the field containing the containing the LSOA code reference (beginning ‘E01’).\n\n\nCode\n# Changing the name of the 'code field'\nnames(Bristol_data)[1] = \"code\"\n\n\nThe ‘code’ field we’ve just created contains too much information, making it impossible to match to Bristol_LSOAs data. The LSOA code is found in the first 9 characters. To shorten the contents here, we can use the substr(...) function:\n\nThe field we want to shorten forms the front of our argument\nx = is the same field\nstart = specifies the start of the crop - here, it is the first character\nstop = specifies the end of the crop - here, it is the ninth character\n\n\n\nCode\n# Shortening the field to the first 9 letters\nBristol_data$code &lt;- substr(x = Bristol_data$code, start = 1, stop = 9)\n\n\nWith the fields properly prepared, we are able to conduct the join using the merge(...) function. Again, note that the geographic data comes first (x=).\n\n\nCode\n# Performing the merge\nBristol_merged &lt;- merge(x = Bristol_LSOAs, y = Bristol_data, by.x = \"lsoa21cd\", by.y = \"code\")\n\n\n\n\n\nWith sf loaded, it is possible to preview the result of the join using the plot function. Let’s examine the first 6 plots using a slicing:\n\n\nCode\nplot(Bristol_merged[,c(1:6)])\n\n\n\n\n\nFigure 1: Preview of merged shapefile data\n\n\n\n\n\n\n\n\nBoundary of the Bristol featured in the LSOA data features the Avonmouth Extension (Figure 1) that protudes out into the sea due the region’s in relation to Bristol’s maritime history that saw shipping trade attributed to the large tidal range of the River Avon that enabled Bristol’s Floating Harbour; shipping traffic falls under the jurisdiction of Bristol’s port authority within the extension. However, we are only interested with the land administration component, where we will need to clip the region accordingly.\n\n\nThe first step is to ‘clip’ the extension off so that we are only looking at the land. This is achieved by downloading the boundaries for Bristol’s wards. The Shapefile version of the data should be downloaded and loaded following a similar process to that described in Section 1.1.2.\nLet’s load the shapefile as an object called ‘Bristol_wards’ (ensuring all of the dependencies are present):\n\n\nCode\nBristol_wards &lt;- read_sf(\"Wards/Wards.shp\")\n\n\nWe can plot one of the variables using the plot(...) function:\n\n\nCode\nplot(Bristol_wards[\"NAME\"])\n\n\n\n\n\nFigure 2: Bristol Wardss layer (NAME field sliced)\n\n\n\n\nThis good, but let’s merge all of the separate polygons into a single landmass. The sf package features a useful function that dissolves boundaries called st_union(...):\n\n\nCode\nBristol_Mass &lt;- st_union(x = Bristol_wards)\nplot(Bristol_Mass, col = \"palegreen\")\n\n\n\n\n\nFigure 3: The dissolved wards layer\n\n\n\n\n\n\n\nNow that we have the dissolved (or unionised) layer, we are in a position to clip off the Avonmouth Extension. Here’s we’ll use the st_clip(...) function and use the Bristol_Mass object as the mask.\n\nx = The data that needs to be cropped (Bristol_merged)\ny = The cropping background\n\n\n\nCode\nBristol_merge_2 &lt;- st_crop(x = Bristol_merged, y = Bristol_Mass)\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nCode\nplot(Bristol_merge_2[\"Social.rented\"])\n\n\n\n\n\nA preview of Bristol’s social rentals (2021) excluding the Avonmouth Extension\n\n\n\n\nAt this point, we are examining continuous data where the blue tones indicate lower observation frequencies, while the yellow tones are the higher frequencies.\n\n\n\n\nInspecting social rentals, we can detect areas where social rentals (i.e. council tenancies) are highest - in the middle and towards the bottom. However, it is difficult to ascertain scale and location, where this graphic lacks cartographic elements and cannot be considered a ‘map’ at this stage. To introduce cartographic elements another package needs to be introduced, tmap (Tennekes 2018). Loading this package follows a similar process to that described above:\n\n\nCode\nif(!require(tmap)) install.packages(\"tmap\")\n\n\nLoading required package: tmap\n\n\nThe parsing for tmap differs from the default R coding structure and closely aligned to an extended graphical suite called ggplot that features in other programming languages such as Python. Here’s the code for producing a basic preview for our clipped data with tmap is as follows:\n\n\nCode\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\")\n\n\n\n\n\nFigure 4: Preview of social rents in Bristol using tmap\n\n\n\n\n\ntm_shape(...) introduces the geographic (mappable data)\ntm_polygons(...) specifies the geometry and its aesthetic properties.\n\nThese are linked together with + symbols.\nTo make this graphic cartographic, we need to include the following elements:\n\nCompass: function is tm_compass(...)\nScale bar: function is tm_scale_bar(...)\nGraticules (longitude and latitude lines): function is tm_graticules(...)\n\nAdding these on can be achieved by using further + symbols in your code. An example is shown in Figure 5.\n\n\nCode\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\")+\n  tm_compass()+\n  tm_scale_bar()+\n  tm_graticules()\n\n\n\n\n\nFigure 5: A map (with cartographic features) produced in tmap\n\n\n\n\n\n\nAs with the plot(...) function, colours and elements can be manipulated. You can learn more about how to customise the look and feel of your map by inserting ? in front of the graphical element that you want to manipulate.\nAn example of how arguments can be applied to improve the map above is as follows:\n\n\nCode\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"navy\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"navy\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\n\n\n\nFigure 6: Various aesthetic manipulations applied to out data\n\n\n\n\nIn addition to the placement, the map’s default colour scheme can also be manipulated. Using either a default Brewer palette, importing a palette library, or by creating your own customised palette. Here’s an example of a map produced using a custom colour palette (Figure 7)\n\n\nCode\ncustom_pal &lt;- c(\"red\",\"grey20\")\n\ntm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0, palette = rev(custom_pal), n = 12)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"white\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"white\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\n\n\n\nFigure 7: Application of a custom colour palette\n\n\n\n\nA critical step in the production of this graphic is the use of palette = argument that points to the custom colour palette. Within this argument the rev(...) function is applied to reverse the colour palette.\n\n\n\n\nWhile someone with a good understanding of geometry can figure out the precise location of the featured regions, for most the inclusion of a base map is far more practical. This can be added using a package called rosm (Dunnington 2022) that has access to Open Street Map map tiles stored in a raster format. We’ll discuss rasters in another section.\n\n\nCode\n# Installing rosm\nif(!require(rosm)) install.packages(\"rosm\")\n\n\nLoading required package: rosm\n\n\n\n\nAs Open Street Map is projected to the Mercator projection (same as Google Maps), we need to change the projection of our map data (Bristol_merge_2) so that it conforms. The EPSG code we need is 4326 This can be achieved using the st_transform(...) from the sf package. Here is how it is applied in the creation of a new object simply called Bristol_reproj:\n\n\nCode\nBristol_reproj &lt;- st_transform(x = Bristol_merge_2, crs = 4326)\n\n\nWith the application of the Mercator projection assigned to our data, we can now retrieve a relevant base map. For this, we need the osm.raster(...) function from rosm and the st_bbox(...) function from sf to define the bounding box (or rectangular extents) of our data.\n\n\nCode\nmy_basemap &lt;- rosm::osm.raster(st_bbox(Bristol_reproj),)\n\n\nZoom: 12\n\n\nTo add the base map, a raster image, we use tm_shape(...) in combination with the tm_rgb(...) function from tmap. Here’s our base map:\n\n\nCode\ntm_shape(my_basemap)+\n  tm_rgb()\n\n\n\n\n\nFigure 8: The retreived rosm basemap\n\n\n\n\nAs with other elements in tmap the basemap can be manipulated. Let’s make it monochrome by lowering the saturation = argument to 0:\n\n\nCode\ntm_shape(my_basemap)+\n  tm_rgb(saturation = 0)\n\n\n\n\n\nFigure 9: The base map in monochrome, saturation set to 0\n\n\n\n\nAdding our mapped data onto of this is this base map is achieved by adding the map code immediately after the base map. Transparency on for out polygon layer is achieved by introducing the alpha = argument, the selected value is 0.45:\n\n\nCode\ntm_shape(my_basemap)+\n  tm_rgb(saturation = 0)+\n  tm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0, palette = rev(custom_pal), n = 12, alpha = 0.45)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"white\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"white\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\n\n\n\nFigure 10: Applying the base map to our mapped data…\n\n\n\n\nAs seen in Figure 10, we are able to contextualise data we’ve imported by seeing the names of nearby areas. Looking at our web browswers it is possible to identify that Hartcliffe and areas close to Easton, Redfield, and St Philips are have high occurrences of social rented housing.\n\n\n\n\nA helpful feature in tmap is its ability to produce interactive web maps. These are essentially rendered using a javascript package known as leaftlet. Creating an interactive map in tmap is achieved by using the following command before your main code tmap_mode(\"view\"). Here’s the result when applied to our map above:\n\n\nCode\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nCode\n#tm_shape(my_basemap)+\n  #tm_rgb(saturation = 0)+\n  tm_shape(shp = Bristol_merge_2)+\n  tm_polygons(col = \"Social.rented\", title = \"Social rented\", border.alpha = 0, palette = rev(custom_pal), n = 12, alpha = 0.45)+\n  tm_compass(position = c(\"right\", \"top\"), color.dark = \"navy\",type = \"4star\", text.color = \"white\")+\n  tm_scale_bar(position = c(\"left\", \"bottom\"), color.dark = \"navy\",text.color = \"white\", breaks = c(0,2,4,6))+\n  tm_graticules(col = \"navy\")+\n  tm_layout(legend.outside = TRUE, legend.position = c(\"right\",\"center\"),\n            bg.color = \"grey\", frame = FALSE)\n\n\nCompass not supported in view mode.\n\n\nlegend.postion is used for plot mode. Use view.legend.position in tm_view to set the legend position in view mode.\n\n\nWarning: In view mode, scale bar breaks are ignored.\n\n\n\n\n\n\nFigure 11: An example of an interactive map\n\n\n\nA crucial concern with this mode of mapping is the loss of cartographic elements including the graticules, and compass - this results in a number of warnings when our raw code is introduced. Since several base maps are supplied by default, we no longer have to generate a base map. To change the mode back, we use the following line of code:\n\n\nCode\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Mapping_1.html#raster-mapping",
    "href": "Mapping_1.html#raster-mapping",
    "title": "Visualising Spatial Data I",
    "section": "2 Raster Mapping",
    "text": "2 Raster Mapping\nSection coming soon"
  },
  {
    "objectID": "Classical_Measures.html#central-tendency-and-spread",
    "href": "Classical_Measures.html#central-tendency-and-spread",
    "title": "Classical Measures",
    "section": "1.4 Central Tendency and Spread",
    "text": "1.4 Central Tendency and Spread\nWhen examining distributions we are immediately drawn to its shape, where peaks indicate higher numbers of frequencies. The best known measure is the median, followed by the arithmetic mean and the mode. The median, mode and mean are particularly useful in describing the shape of a distribution without a graphic aid.\n\nWhen the mean has a greater value than a median (and/or the mode is less than the median), the shape of the distribution is positively skewed; the distribution has a tail towards the right.\nWhen the mean is less than the median (and/or the mode is greater than the median), the shape of the distribution is negatively skewed, with a long tail to the left.\nWhen the mean, mode, and median are aligned, the shape of the distribution is symmetrical.\n\nBy way of illustration see Figure 1.\n\n\nnull device \n          1 \n\n\n\n\n\nFigure 1: Positve and negative skews generated by random beta distributions. Median is red while the mean is shown by a blue dashed line\n\n\n\n\nThe spread of a sample’s distribution is determined by how far the values within a distribution deviate from the sample’s mean (\\(\\bar{x}\\)). Standard deviation is a standardisation of variance, but what do we mean by variance.\n\n1.4.1 A description of variance\nSample variance in essence are an index of values and their distance from a sample mean. Its mathematical symbol is \\(\\sigma^2\\). It’s formula is as follows:\n\\[\n\\sigma^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1}\n\\tag{5}\\]\nFor our sample, we need to find the total sum of squares (\\(TSS\\), see Equation 6). The reason for the power function is to remove the negative signs that will return erroneous results.\n\\[\nTSS = \\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\tag{6}\\]\nThe final stage of the equation (as shown in Equation 7) is to divide the observations by the number of observations in your sample, resulting in an average. Note the use of \\(n-1\\) instead of just \\(n\\); this indicates the number of independent observations that are able to vary. A sample of 100 observations is able to vary up to its 99th observation where, at the 100th value, variance is lost. \\(n-1\\) is referred to as the sample’s degree’s of freedom (\\(\\nu\\) or ‘df’).\n\n\n1.4.2 Standard deviation as measure of spread\nWhile variance (\\(\\sigma^2\\)) give us an indication of differences of values from the mean, the problem is that its result is too large to relate directly back to the original sample. To address this issue, we introduce a square root function to the overall equation so that \\(\\sigm^2\\) becomes \\(\\sigma\\). The equation is as follows:\n\\[\n\\sigma = \\sqrt\\frac{\\sum(x_i-\\bar{x})^2}{n-1}\n\\tag{7}\\]\nWhen this figure is calculated, we are able to determine the spread of a distribution given the standard deviation of observations from the sample’s overall mean.\nCalculating the standard deviation of a vector (or variable) is straightforward in R via the sd(...) function. Let’s use the sd(...) function to work out the spread of a randomly generated distribution.\n\n\nCode\nset.seed(45)\n\npos_dist &lt;- rbeta(100, 1, 10)\nst_dev &lt;- sd(pos_dist)\n\nplot(density(pos_dist, adjust = 5), main = \"Postive skew\")\nabline(v = mean(pos_dist), col = \"blue\", lty = 3)\nabline(v = mean(pos_dist)+st_dev, col = \"green\")\nabline(v = mean(pos_dist)-st_dev, col = \"green\")\n\n\n\n\n\nFigure 2: Random distribution showing the mean (blue dashed line) and standard deviation range (green lines)\n\n\n\n\nBased on this example we can see that most of our is a distance of ~0.08 on either side of the mean, giving us further insights regarding our data distribution in addition to there interquartile range, mode, mean, and medians."
  },
  {
    "objectID": "Classical_Measures.html#central-tendency",
    "href": "Classical_Measures.html#central-tendency",
    "title": "Classical Measures",
    "section": "1.4 Central Tendency",
    "text": "1.4 Central Tendency\nWhen examining distributions we are immediately drawn to its shape, where peaks indicate higher numbers of frequencies. The best known measure is the median, followed by the arithmetic mean and the mode. The median, mode and mean are particularly useful in describing the shape of a distribution without a graphic aid.\n\nWhen the mean has a greater value than a median (and/or the mode is less than the median), the shape of the distribution is positively skewed; the distribution has a tail towards the right.\nWhen the mean is less than the median (and/or the mode is greater than the median), the shape of the distribution is negatively skewed, with a long tail to the left.\nWhen the mean, mode, and median are aligned, the shape of the distribution is symmetrical.\n\nBy way of illustration see Figure 1.\n\n\n\n\n\nFigure 1: Positve and negative skews generated by random beta distributions. Median is red while the mean is shown by a blue dashed line\n\n\n\n\n\n1.4.1 Gaussian normal distributions\nA symmetric, bell-shaped distribution with a single peak. An example can be simulated using the rnorm() function. This function returns a random numerical vector with properties that form to a Gaussian normal distribution Figure 2.\n\n\nCode\nset.seed(13)\n\n# Generating the distribution...\nnorm_dist &lt;- rnorm(100, mean = 10, sd = 1)\n\n# Plotting the result...\nplot(density(x = norm_dist, adjust = 5), main = \"Normal distribution\")\nabline(v = median(norm_dist), col = \"red\")\nabline(v = mean(norm_dist), col = \"blue\", lty = 2)\nlegend(14, 0.15, legend = c(\"Median\",\"Mean\"),\n       col = c(\"red\",\"blue\"), lty = 1:2)\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nUnder these conditions you are able to apply a range of probability analyses, working with skew data posses limitations in this regard, where researchers often strive to convert their data to this distribution by either removing outliers or by transforming its values - a process discussed much later on.\n\n\n1.4.2 Dealing with outliers\nOutliers in a variable are observations that are considered extreme. They are exceptionally high or exceptionally high relative to the rest of the data present, specifically the interquartile range hinges (Tukey 1977). Conventionally, they can play a role in Centrality of a distribution, where removing them could aid in the reliability of your analysis. They can be conventionally detected by applying Equation 5 to find lower outliers and Equation 6 to find higher outliers.\n\\[\nLO = Q1 - (1.5 \\times IQR)\n\\tag{5}\\]\n\\[\nHO = Q3 + (1.5 \\times IQR)\n\\tag{6}\\]\nOutliers are typically depicted in box whisker plots, where in R it is possible to retrieve numerical values using the boxplot.stats(...) function to access the metadata behind the plot. Let’s apply this function to some data concerning the distribution of Christians in Thanet, South East England.\n\n\nCode\n# Acquiring the data...\nThanet_religion &lt;- read.csv(\"https://raw.githubusercontent.com/Richtea84/I2Q-files/main/Thanet.csv\")\n\n# Box whisker plot for the region's Buddhists...\nboxplot(x = Thanet_religion$Christian, horizontal = TRUE,\n        col = \"palegreen\")\n\n\n\n\n\nDistribution of Christians in Thanet\n\n\n\n\nWhen the boxplot.stats(...) function is applied, the following result is rendered:\n\n\nCode\nboxplot.stats(Thanet_religion$Christian)\n\n\n$stats\n[1]  685.0  886.5  954.5 1071.0 1300.0\n\n$n\n[1] 84\n\n$conf\n[1] 922.6937 986.3063\n\n$out\n[1] 1515  594 1363 1478 1386\n\n\n\nThe stats are, in order, the extreme value of the lower whisker, the lower ‘hinge’ where the first 25% of the observations fall, the median, the upper hinge where the last 25% of the data falls, and the extreme of the upper whisker\nn is the size of the same. In Thanet, there are 84 lower super output areas\nconf are extremes of the confidence interval (at 95%). Confidence intervals are discussed later on.\nout returns values that are beyond the extreme whiskers. These are the outliers.\n\nTo isolate the outliers, the code is as follows:\n\n\nCode\nboxplot.stats(Thanet_religion$Christian)$out\n\n\n[1] 1515  594 1363 1478 1386\n\n\nTo remove these outliers from the data frame, you can use the which(...) function in R. This is how it is applied to the Thanet data:\n\n\nCode\n# Number of rows in original Thanet data\nprint(paste(\"Number of rows in the original Thanet data set = \",nrow(Thanet_religion),sep = \"\"))\n\n\n[1] \"Number of rows in the original Thanet data set = 84\"\n\n\nCode\n# Storing the identified outliers\nOutliers &lt;- boxplot.stats(Thanet_religion$Christian)$out\n\n# Removing the outliers\nThanet_NO &lt;- Thanet_religion[-which(Thanet_religion$Christian %in%\n                     c(Outliers)),]\n\nprint(paste(\"Number of rows in the adjusted Thanet data set = \",nrow(Thanet_NO),sep = \"\"))\n\n\n[1] \"Number of rows in the adjusted Thanet data set = 79\"\n\n\nIn essence, the which(...) function returns a position that is based on a condition; data which can be described in a certain way. Here, it is being used to slice the Thanet_religion data frame to find the number of Christians that are considered outliers. Specifically, -which(...) is being used to invert the process, so that we are finding all of the records that are not considered outliers.\n\n\n\n\n\n\nTip\n\n\n\nRemember that the $ operator is used to access lists within a data frame. I works in similar way to square brackets [] that can also be used when slicing a data frame. The $ cannot be used to slice a matrix.\nThe %in% operator in R pushes code from one process to another. In the Thanet example above, the Christian data is being pushed into a concatenated list of outliers.\n\n\nThe distribution of Christian in Thanet can be seen in Figure 2. The distribution shows a visible positive skew. This is confirmed by mean value being greater than the median value. In Figure 3, the symmetry of the distribution appears to have improved, but a positive skew is still detectable; the difference between the mode and median has reduced, a significant outcome that will play a role in later analyses.\n\n\nCode\nhist(Thanet_religion$Christian, col = \"palegreen\",\n     main = \"Original Data\", xlab = \"No. of Christians\", xlim = c(300, 1700))\nabline(v = median(Thanet_religion$Christian), col = \"red\")\nabline(v = mean(Thanet_religion$Christian), col = \"blue\", lty = 2)\nlegend(1400, 15, legend = c(\"Median\",\"Mean\"),\n       col = c(\"red\",\"blue\"), lty = 1:2)\n\n\n\n\n\nFigure 3: ?(caption)\n\n\n\n\n\n\nCode\nhist(Thanet_NO$Christian, col = \"palegreen\",\n     main = \"Adjusted Data\", xlab = \"No. of Christians\", xlim = c(300, 1700))\nabline(v = median(Thanet_NO$Christian), col = \"red\")\nabline(v = mean(Thanet_NO$Christian), col = \"blue\", lty = 2)\nlegend(1400, 15, legend = c(\"Median\",\"Mean\"),\n       col = c(\"red\",\"blue\"), lty = 1:2)\n\n\n\n\n\nFigure 4: ?(caption)\n\n\n\n\n\n\n1.4.3 The trimmed mean\nLet’s a imagine a situation where outliers cannot be removed or, when they are, no difference is made to the distribution of the data. Your mean average is affected by this situation. An option is to trim the mean, where the contents of the variable are sorted into a numeric order and a portion sliced off (see Equation 7)\n\\[\n\\bar{x}_t = \\frac{\\sum_{i=p+1}^{n-p}x_i}{n-2p}\n\\tag{7}\\]\nIn Equation 7, \\(p\\) refers to the portion. Where the we are generating an average for section of data where the last portion has been removed (\\(n-p\\)); the indexing duly states that the starting point relative to the index is the original value plus the proportion. The result is a trimming of the variable.\nTo work out the estimate, the trimmed sample must be divided by the size of the trimmed sample from both ends: \\(n-2p\\).\nCalculating the trimmed mean is fairly straightforward in R by adding the trim = argument to the mean(...) function. Here the trim is the desired proportion. Entering a value of 0.25 will remove the top and bottom 25% of the data’s observations. Let’s try this with our Thanet data, this time looking at the number of Muslims.\n\n\nCode\nmean(x = Thanet_religion$Muslim, trim = 0.25)\n\n\n[1] 11.47619\n\n\nThe original mean is 14.6428571.\nAs with the removal of outliers, the context underwhich you will trim the mean depends on the specifics of your analysis and the shape of the data concerned."
  },
  {
    "objectID": "Classical_Measures.html#spread",
    "href": "Classical_Measures.html#spread",
    "title": "Classical Measures",
    "section": "1.5 Spread",
    "text": "1.5 Spread\nThe spread of a sample’s distribution is determined by how far the values within a distribution deviate from the sample’s mean (\\(\\bar{x}\\)). Standard deviation is a standardisation of variance, but what do we mean by variance.\n\n1.5.1 A description of variance\nSample variance in essence are an index of values and their distance from a sample mean. Its mathematical symbol is \\(\\sigma^2\\). It’s formula is as follows:\n\\[\n\\sigma^2 = \\frac{\\sum(x_i-\\bar{x})^2}{n-1}\n\\tag{8}\\]\nIn essence, what we are doing is finding the total numerical distance of each observation within of a sample \\(x\\) from the sample’s mean \\(\\bar{x}\\). For our sample, we need to find the total sum of squares (\\(TSS\\), see Equation 9). The reason for the power function is to remove the negative signs that will return erroneous results.\n\\[\nTSS = \\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\tag{9}\\]\nThe final stage of the equation (as shown in Equation 8) is to divide the observations by the number of observations in your sample, resulting in an average. Note the use of \\(n-1\\) instead of just \\(n\\); this indicates the number of independent observations that are able to vary. A sample of 100 observations is able to vary up to its 99th observation where, at the 100th value, variance is lost. \\(n-1\\) is referred to as the sample’s degree’s of freedom (\\(\\nu\\) or ‘df’).\n\n\n1.5.2 Standard deviation as measure of spread\nWhile variance (\\(\\sigma^2\\)) gives us an indication of differences of values from the mean, the problem is that its result is too large to relate directly back to the original sample. To address this issue, we introduce a square root function to the overall equation so that \\(\\sigm^2\\) becomes \\(\\sigma\\). The equation is as follows:\n\\[\n\\sigma = \\sqrt\\frac{\\sum(x_i-\\bar{x})^2}{n-1}\n\\]\nWhen this figure is calculated, we are able to determine the spread of a distribution given the standard deviation of observations from the sample’s overall mean (see Figure 5).\n\n\n\n\n\nFigure 5: Example of normal distribution with the number of standard deviations.\n\n\n\n\nCalculating the standard deviation of a vector (or variable) is straightforward in R via the sd(...) function. Let’s use the sd(...) function to work out the spread of a randomly generated distribution.\n\n\nCode\nset.seed(45)\n\npos_dist &lt;- rbeta(100, 1, 10)\nst_dev &lt;- sd(pos_dist)\n\nplot(density(pos_dist, adjust = 5), main = \"Postive skew\")\nabline(v = mean(pos_dist), col = \"blue\", lty = 3)\nabline(v = mean(pos_dist)+st_dev, col = \"green\")\nabline(v = mean(pos_dist)-st_dev, col = \"green\")\n\n\n\n\n\nFigure 6: Random distribution showing the mean (blue dashed line) and standard deviation range (green lines)\n\n\n\n\nBased on this example we can see that most of the standard deviation of our sample’s is ~0.08 on either side of the mean, giving us further insights regarding our data distribution in addition to there interquartile range, mode, mean, and medians."
  },
  {
    "objectID": "Classical_Measures.html#probability-functions",
    "href": "Classical_Measures.html#probability-functions",
    "title": "Classical Measures",
    "section": "1.6 Probability functions",
    "text": "1.6 Probability functions\nProbability relates to the chance of something happening. When it comes to statistics, probability functions can tell us the likelihood of observing particular results from our data variable.\nProbability functions are most accurate when data is normally distributed. Eyeballing graphs or considering marginal differences between the mean, mode, and median can give an indication of normality, however, there are specific tests that can help us to readily identify whether or not a variable is normally distributed.\n\n1.6.1 Z-scores\nA useful aid in probability analysis are Z standard scores (or Z-scores). In itself, a z-score is useful for calculating how many standard deviations an observation is from its sample mean. These are calculated by subtracting an obsevation (\\(x\\)) from its sample mean (\\(\\mu\\)) and dividing the result by its the sample standard deviation (\\(\\sigma\\)) see Equation 10.\n\\[\nZ = \\frac{x-\\mu}{\\sigma}\n\\tag{10}\\]\nCalculating Z-scores in R is relatively straight forward using the mean(...) and sd(...) functions. To work out the z-scores for Christians in the first 10 Thanet_NO data observations, we can use the following code:\n\n\nCode\nhead(\n  (Thanet_NO$Christian - mean(Thanet_NO$Christian))/sd(Thanet_NO$Christian),\n  10\n  )\n\n\n [1]  1.1310132  2.4006156 -0.1314565  0.4819469  0.5889358  1.2807978\n [7]  0.8100464  0.2608364  1.6160299 -0.5380145\n\n\n\n\n\n\n\n\nQuick note on user-defined functions\n\n\n\nThe process for calculating Z-scores in R is rather extrpolated. If you wanted to reduce the inputs, you can consider building you own user defined function. This is enabled using the the function(...) function in R. function(...) only requires vector(s) for input(s), where the curly braces define the treatment of your defined vector/input. the formula. To build a custom process for Z-scores, the process is as follows:\n\n\nCode\nz_scores &lt;- function(x){\n  result = (x - mean(x))/sd(x)\n  return(result)\n  }\n\n\n\n\nWhen this custom function is applied to our data, the results are as follows:\n\n\nCode\nhead(z_scores(x = Thanet_NO$Christian),10)\n\n\n [1]  1.1310132  2.4006156 -0.1314565  0.4819469  0.5889358  1.2807978\n [7]  0.8100464  0.2608364  1.6160299 -0.5380145\n\n\n\n\n1.6.2 Q-Q plots\nIn addition to histograms and KDE plots, Quantile-Quantile (plots) are able to indicate whether or not data is normally distributed. In essence, it is a 2-dimensional plot that compares observed quantiles (\\(y\\)) with theoretical quantiles (\\(\\hat{y}\\)).\nTheoretical quantiles are derived from Z-scores for both your data and those found in an equivalently sized standard normal distribution. To calculate this you’ll need to work out the proportion for each observation’s Z-score. To do this, divide the values by the number of observations. As we’re dealing with variance, we subtract 1 from each half of the fraction \\(p = \\frac{x-1}{n-1}\\). For demonstration purposes, let’s construct a data frame for the first observations noted above:\n\n\nCode\ndata &lt;- sort(head(Thanet_NO$Christian,10))\n\nz_scores &lt;- (data - mean(data))/sd(data)\nz_scores &lt;- (as.data.frame(z_scores))\n\n# Simpler header...\nnames(z_scores)[1] &lt;- \"Sorted Z-scores\"\n\n# Proportionality...\nz_scores$Prop &lt;- 1:nrow(z_scores)/(1+nrow(z_scores))\nz_scores$Prop &lt;- round(z_scores$Prop,4)\n\nnames(z_scores)[2] &lt;- \"Proportion\"\n\n# Preview the results...\nknitr::kable(t(z_scores))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSorted Z-scores\n-1.543638\n-1.071096\n-0.615134\n-0.3581373\n-0.2337841\n0.0232126\n0.3962723\n0.5703669\n0.960007\n1.871931\n\n\nProportion\n0.090900\n0.181800\n0.272700\n0.3636000\n0.4545000\n0.5455000\n0.6364000\n0.7273000\n0.818200\n0.909100\n\n\n\n\n\nUsing these proportions, you are able to retrieve equivalent Z-scores from a normal distribution (i.e. your theoretical quantiles). In R, this can be achieved by generating a similarly sized random distribution and querying it with the qnorm(...) function that relies on a standard normal distribution. Here:\n\np = will be the Proportions we’ve just calculated\nmean = will be 0\nsd = will be 1\n\nThe result can be previewed using the plot(...) function.\n\n\nCode\nz_normal &lt;- qnorm(p = z_scores$Proportion, mean = 0, sd = 1)\nplot(x = z_normal, y = data, xlab = \"Theoretical Qauntiles\", ylab = \"Observed Quantiles\",\n     main = \"Manually produced Q-Q plot\")\n\n\n\n\n\nFigure 7: ?(caption)\n\n\n\n\nIn practice, you’d use the qqnorm(...) function instead, where you merely supply the \\(y\\) variable you are interested in. In practice is a good idea to include the predictor line in such a plot. This can be achieved using the qqline(...) function and supplying the same arguments (see Figure 7).\n\n\nCode\nqqnorm(y = head(Thanet_NO$Christian,10))\nqqline(y = head(Thanet_NO$Christian,10), col = \"red\")\n\n\n\n\n\nFigure 8: ?(caption)\n\n\n\n\nWhen the points fall along the line of a Q-Q plot, a normal distribution is present in the data sample. A Q-Q for all of Thanet’s Christians in 2011 (excluding outliers) can be seen in Figure 8. The points loosely follow the trajectory of the line and approximate normalcy can be argued.\nThe skewness of a distribution can also be ascertained from a Q-Q plot. When the points fall away from the line towards the negative numbers, then we have a negative skew; a positive skew occurs when the points move away from the line as the quantile values increase.\n\n\nCode\nqqnorm(Thanet_NO$Christian)\nqqline(Thanet_NO$Christian, col = \"red\")\n\n\n\n\n\nFigure 9: ?(caption)\n\n\n\n\n\n\n1.6.3 The Shapiro-Wilk test\nA reliable way to test for normality prior to using probability functions is the Shapiro-Wilk test. For reference, it’s equation is as follows:\n\\[\nW = \\frac{\\left( \\sum_{i=1}^{n}a_i x_{(i)} \\right )^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\tag{11}\\]\nAn easy way of thinking about Equation 11, is as a precise ‘fit’ coefficient statistic for a Q-Q plot. The top half of the equation squares the sum of normal value that has been matched against its ordered x equivalent. This is in turn divided by the sum of squares that returns the \\(W\\) statistic.\nIn essence, the \\(W\\) statistic tells us how closely our distribution’s shape aligns to a normal distribution. Once can argue that a value that is &gt;0.5 can be considered normal, however, using this statistic alone will mean that a value of \\(W = 1\\) will be the only confirmation of normalcy in the data distribution.\nLet’s go back to our value 0.5 (let’s call it \\(W'\\)), and assume that we can see that the distribution appears normal by looking at either the distributions histogram or Q-Q plot, the only way we can confirm normality is by examining the likelihood of seeing this statistic against its critical value within a predetermined confidence interval. Consider ‘Table 2’ from the following web resource: https://real-statistics.com/statistics-tables/shapiro-wilk-table/. For a sample with 10 observations, at 95% confidence (an alpha of 0.05), the critical value is ~0.842. As our value of \\(W' &lt; W\\), we conclude that our distribution does not conform to a normal distribution.\nWe can easily perform this test in R using the shapiro.test(...) function. This function is particularly useful as it matches the resulting \\(W\\) statistic against a probability value. However, the understanding is that if your p-value is less than a predetermined confidence interval’s , then the null-hypothesis is rejected where normalcy cannot be confirmed. If it is above the , then we can assume normalcy and consider the data as parametric.\nLet’s apply a Shapiro-Wilk test to our Thanet data (Christians).\n\n\nCode\nshapiro.test(x = Thanet_NO$Christian)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Thanet_NO$Christian\nW = 0.98525, p-value = 0.498\n\n\nFrom this result it is apparent that the \\(W\\) value is quite high at 0.99, the p-value is also above an of 0.05 (0.5). Therefore we can conclude that the distribution is normal. Let’s consider the Muslim population:\n\n\nCode\nshapiro.test(x = Thanet_NO$Muslim)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Thanet_NO$Muslim\nW = 0.84917, p-value = 1.744e-07\n\n\nAgain, the \\(W\\) value suggests a normal shape to our distribution, but the p-value is less than an of 0.05 (0.0000001743786). We conclude that the data here is not normally distributed (or nonparametric).\n\n\n\n\n\n\nNote\n\n\n\nIn R Shapiro-Wilk tests can be applied to up to 5,000 observations. Larger sample sizes will not compute because of limitations in the synthesis of critical values associated with the principle method.\n\n\n\n\n1.6.4 The Anderson-Darling test\nIf you found yourself in a situation where you are not fully convinced by the results of your Shapiro-Wilk test, an alternative is the Anderson-Darling test.\nThis goodness-of-fit test is based on mathematical integration for two distributions:\n\\[\nAD = n\\int_{\\infty}^{\\infty}\\frac{\\left ( F_n(x)-F(x) \\right )^2}{F(x) \\left ( 1 -F(x) \\right )} dF(x)\n\\tag{12}\\]\nin Equation 12, \\(F_n(x)\\) is the empirical distribution and \\(F(x)\\) is the reference distribution (Anderson and Darling 1954). Effectively, for an number of observations along a distribution and its respective cumulative equivalent, you are 1) subtracting the values of the reference distribution from your observed distribution and dividing the value by the reference distribution (multiplied by its degrees of freedom). This result is then multiplied by degrees of freedom for your number of observations \\(dF(x)\\). Again, this statistic is compared against a critical value to determine normalcy. If the value is above the critical value, normalcy can be assumed; if below, then normality can be rejected.\nHelpfully, there is a package in R that does the hard work for us called nortest. Below is how it can be applied to confirm the non-normal distribution of Thanet’s muslim population.\n\n\nCode\nif(!require(nortest)) install.packages(\"nortest\")\n\nad.test(x = Thanet_NO$Muslim)\n\n\n\n    Anderson-Darling normality test\n\ndata:  Thanet_NO$Muslim\nA = 3.6933, p-value = 2.673e-09\n\n\nIn the result, p-value is interpreted in the same way as for a Shapiro-Wilk test. That is, as it is below an \\(\\alpha\\) of 0.05, we conclude that the difference between the distributions is statistically significant, where we can conlcude that the distribution is not normal.\n\n\n\n\n\n\nNote\n\n\n\nTo my knowledge, there is no maximum number of observations that can be applied to a Anderson-Darling test. However, it is generally advised that it be applied to a minimum of 20 observations.\n\n\n\n\n1.6.5 The Kolgomorov-Smirnov Test\nAn alternative to the Anderson-Darling test is the Kolmogorov-Smirnov test. The advantage here is that you are able to compare your distribution to other theoretical data distribution including the normal distribution, Poisson distribution and binomial distribution.\nThis is particularly powerful when determining your instruments for analysis and overall methodical approach going forward.\nThe critical statistic for this test is \\(D\\) (distance), indicating how far your sample is from another distribution. It is calculated in the following way:\n\\[\nD = max | F_o(x)-F_r(x)|\n\\tag{13}\\]\nConceptually, the \\(F_o(x)\\) is a cumulative frequency distribution of sample of observations; \\(F_r(x)\\) is the theoretical frequency distribution. You can be more specific with \\(F_o(x)\\) by taking the number of observations that are above a given value (slicing), and dividing these by total number of observations. In theory you can match your sample against other theoretical distributions such as Poisson or Binomial.\nIn R, implemented a Kolmogorov-Smirnov test is relatively straightforward with the ks.test(...) function. Let’s use it to check the normalcy of the distribution for Thanet’s muslims:\n\n\nCode\nks.test(x = Thanet_NO$Muslim, y = \"dnorm\")\n\n\nWarning in ks.test.default(x = Thanet_NO$Muslim, y = \"dnorm\"): ties should not\nbe present for the Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  Thanet_NO$Muslim\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nThe argument x = specifies the location of our observations (data) while the y = argument indicates the reference distribution. In the example featured above, we are looking at the normal distribution as rendered by R itself (dnorm).\nHere, the \\(D\\) value is quite high, indicating a greater distance of an observation from its matched distribution. The low p-value confirms that the distribution of our data does not fall within a normal distribution.\nThis test is best suited for larger data sets and is prone to return erroneous results (Type-I and Type-II errors) when examining smaller samples.  Use with caution and critically evaluate the results!."
  }
]